2025-12-05 02:22:43,970: Requested GPU 1 not available. Using GPU 0 instead.
2025-12-05 02:22:44,122: Using device: cuda:0
2025-12-05 02:22:45,080: Using torch.compile
2025-12-05 02:22:46,353: Initialized RNN decoding model
2025-12-05 02:22:46,353: OptimizedModule(
  (_orig_mod): GRUDecoder(
    (input_layer_dropout): Dropout(p=0.2, inplace=False)
    (gru): GRU(7616, 768, num_layers=5, batch_first=True, dropout=0.4)
    (out): Linear(in_features=768, out_features=41, bias=True)
  )
)
2025-12-05 02:22:46,353: Model has 33,527,849 parameters
2025-12-05 02:22:47,052: Using time embeddings with dimension: 32
2025-12-05 02:22:50,155: Successfully initialized datasets
2025-12-05 02:22:55,183: Train batch 0: loss: 619.84 grad norm: 396.81 time: 4.048
2025-12-05 02:23:23,794: Train batch 20: loss: 478.07 grad norm: 843.08 time: 1.382
2025-12-05 02:23:52,238: Train batch 40: loss: 115.08 grad norm: 350.27 time: 1.577
2025-12-05 02:24:21,195: Train batch 60: loss: 81.68 grad norm: 42.78 time: 1.303
2025-12-05 02:24:49,734: Train batch 80: loss: 77.94 grad norm: 28.28 time: 1.338
2025-12-05 02:25:18,557: Train batch 100: loss: 75.84 grad norm: 55.67 time: 1.341
2025-12-05 02:25:47,384: Train batch 120: loss: 74.14 grad norm: 90.85 time: 1.256
2025-12-05 02:26:16,456: Train batch 140: loss: 66.25 grad norm: 67.44 time: 1.385
2025-12-05 02:26:45,508: Train batch 160: loss: 48.97 grad norm: 30.22 time: 1.434
2025-12-05 02:27:15,013: Train batch 180: loss: 48.56 grad norm: 28.03 time: 1.710
2025-12-05 02:27:44,551: Train batch 200: loss: 40.45 grad norm: 29.64 time: 1.636
2025-12-05 02:28:13,938: Train batch 220: loss: 43.28 grad norm: 30.06 time: 1.454
2025-12-05 02:28:42,194: Train batch 240: loss: 36.93 grad norm: 30.07 time: 1.393
2025-12-05 02:29:10,867: Train batch 260: loss: 35.86 grad norm: 25.27 time: 1.285
2025-12-05 02:29:39,598: Train batch 280: loss: 37.79 grad norm: 29.29 time: 1.374
2025-12-05 02:30:08,461: Train batch 300: loss: 31.91 grad norm: 27.16 time: 1.611
2025-12-05 02:30:37,854: Train batch 320: loss: 30.48 grad norm: 29.77 time: 1.297
2025-12-05 02:31:07,145: Train batch 340: loss: 32.12 grad norm: 30.23 time: 1.249
2025-12-05 02:31:35,961: Train batch 360: loss: 28.23 grad norm: 30.83 time: 1.454
2025-12-05 02:32:04,555: Train batch 380: loss: 23.92 grad norm: 29.10 time: 1.183
2025-12-05 02:32:32,965: Train batch 400: loss: 25.17 grad norm: 32.02 time: 1.372
2025-12-05 02:33:01,341: Train batch 420: loss: 20.22 grad norm: 32.33 time: 1.389
2025-12-05 02:33:30,249: Train batch 440: loss: 24.28 grad norm: 29.13 time: 1.570
2025-12-05 02:33:58,273: Train batch 460: loss: 18.53 grad norm: 26.54 time: 1.472
2025-12-05 02:34:26,109: Train batch 480: loss: 22.17 grad norm: 33.80 time: 1.489
2025-12-05 02:34:56,099: Train batch 500: loss: 26.37 grad norm: 35.16 time: 1.479
2025-12-05 02:34:56,102: Running test after training batch: 500
2025-12-05 02:35:08,594: Val batch 500: PER (avg): 0.2145 CTC Loss (avg): 18.8915 time: 12.492
2025-12-05 02:35:08,594: t15.2023.08.13 val PER: 0.2588
2025-12-05 02:35:08,595: t15.2023.08.18 val PER: 0.2137
2025-12-05 02:35:08,595: t15.2023.08.20 val PER: 0.1962
2025-12-05 02:35:08,595: t15.2023.08.25 val PER: 0.1807
2025-12-05 02:35:08,595: t15.2023.08.27 val PER: 0.2685
2025-12-05 02:35:08,595: t15.2023.09.01 val PER: 0.1753
2025-12-05 02:35:08,595: t15.2023.09.03 val PER: 0.2530
2025-12-05 02:35:08,596: t15.2023.09.24 val PER: 0.2209
2025-12-05 02:35:08,596: t15.2023.09.29 val PER: 0.2026
2025-12-05 02:35:08,596: t15.2023.10.01 val PER: 0.2231
2025-12-05 02:35:08,596: t15.2023.10.06 val PER: 0.2013
2025-12-05 02:35:08,596: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 02:35:08,596: t15.2023.10.13 val PER: 0.2081
2025-12-05 02:35:08,596: t15.2023.10.15 val PER: 0.2138
2025-12-05 02:35:08,597: New best test PER inf --> 0.2145
2025-12-05 02:35:08,597: Checkpointing model
2025-12-05 02:35:09,220: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 02:35:38,014: Train batch 520: loss: 20.65 grad norm: 33.53 time: 1.823
2025-12-05 02:36:06,666: Train batch 540: loss: 18.73 grad norm: 31.29 time: 1.348
2025-12-05 02:36:35,275: Train batch 560: loss: 19.27 grad norm: 33.60 time: 1.325
2025-12-05 02:37:03,137: Train batch 580: loss: 18.54 grad norm: 31.53 time: 1.318
2025-12-05 02:37:32,234: Train batch 600: loss: 18.64 grad norm: 33.46 time: 1.210
2025-12-05 02:38:01,457: Train batch 620: loss: 19.67 grad norm: 46.67 time: 1.082
2025-12-05 02:38:30,748: Train batch 640: loss: 14.08 grad norm: 30.27 time: 1.594
2025-12-05 02:39:00,794: Train batch 660: loss: 16.79 grad norm: 31.76 time: 1.393
2025-12-05 02:39:31,012: Train batch 680: loss: 17.48 grad norm: 33.61 time: 1.843
2025-12-05 02:40:00,606: Train batch 700: loss: 16.49 grad norm: 33.21 time: 1.481
2025-12-05 02:40:28,753: Train batch 720: loss: 16.57 grad norm: 34.35 time: 1.408
2025-12-05 02:40:55,901: Train batch 740: loss: 15.96 grad norm: 27.83 time: 1.402
2025-12-05 02:41:24,563: Train batch 760: loss: 14.72 grad norm: 31.99 time: 1.231
2025-12-05 02:41:53,203: Train batch 780: loss: 15.27 grad norm: 30.68 time: 1.266
2025-12-05 02:42:20,940: Train batch 800: loss: 12.39 grad norm: 26.69 time: 1.254
2025-12-05 02:42:49,673: Train batch 820: loss: 12.94 grad norm: 30.49 time: 1.622
2025-12-05 02:43:18,034: Train batch 840: loss: 15.08 grad norm: 35.53 time: 1.492
2025-12-05 02:43:47,194: Train batch 860: loss: 11.96 grad norm: 29.99 time: 1.386
2025-12-05 02:44:16,504: Train batch 880: loss: 14.42 grad norm: 34.95 time: 1.215
2025-12-05 02:44:46,022: Train batch 900: loss: 11.97 grad norm: 31.27 time: 1.600
2025-12-05 02:45:14,628: Train batch 920: loss: 12.83 grad norm: 32.37 time: 1.217
2025-12-05 02:45:43,101: Train batch 940: loss: 13.28 grad norm: 33.12 time: 1.483
2025-12-05 02:46:10,936: Train batch 960: loss: 13.79 grad norm: 36.08 time: 1.481
2025-12-05 02:46:38,032: Train batch 980: loss: 11.13 grad norm: 28.05 time: 1.242
2025-12-05 02:47:06,922: Train batch 1000: loss: 12.03 grad norm: 37.02 time: 1.409
2025-12-05 02:47:06,924: Running test after training batch: 1000
2025-12-05 02:47:13,842: Val batch 1000: PER (avg): 0.1430 CTC Loss (avg): 13.1233 time: 6.917
2025-12-05 02:47:13,842: t15.2023.08.13 val PER: 0.1632
2025-12-05 02:47:13,842: t15.2023.08.18 val PER: 0.1333
2025-12-05 02:47:13,842: t15.2023.08.20 val PER: 0.1517
2025-12-05 02:47:13,843: t15.2023.08.25 val PER: 0.1024
2025-12-05 02:47:13,843: t15.2023.08.27 val PER: 0.1897
2025-12-05 02:47:13,843: t15.2023.09.01 val PER: 0.1047
2025-12-05 02:47:13,843: t15.2023.09.03 val PER: 0.1912
2025-12-05 02:47:13,843: t15.2023.09.24 val PER: 0.1614
2025-12-05 02:47:13,843: t15.2023.09.29 val PER: 0.1190
2025-12-05 02:47:13,843: t15.2023.10.01 val PER: 0.1384
2025-12-05 02:47:13,844: t15.2023.10.06 val PER: 0.1324
2025-12-05 02:47:13,844: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 02:47:13,844: t15.2023.10.13 val PER: 0.1371
2025-12-05 02:47:13,844: t15.2023.10.15 val PER: 0.1449
2025-12-05 02:47:13,844: New best test PER 0.2145 --> 0.1430
2025-12-05 02:47:13,844: Checkpointing model
2025-12-05 02:47:17,538: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 02:47:47,407: Train batch 1020: loss: 10.45 grad norm: 29.96 time: 1.303
2025-12-05 02:48:16,721: Train batch 1040: loss: 12.24 grad norm: 33.39 time: 1.832
2025-12-05 02:48:45,587: Train batch 1060: loss: 10.60 grad norm: 30.23 time: 1.278
2025-12-05 02:49:13,856: Train batch 1080: loss: 12.28 grad norm: 31.79 time: 1.398
2025-12-05 02:49:41,663: Train batch 1100: loss: 8.25 grad norm: 26.35 time: 1.426
2025-12-05 02:50:09,609: Train batch 1120: loss: 10.40 grad norm: 37.22 time: 1.587
2025-12-05 02:50:38,515: Train batch 1140: loss: 14.91 grad norm: 41.99 time: 1.513
2025-12-05 02:51:07,863: Train batch 1160: loss: 9.42 grad norm: 31.87 time: 1.381
2025-12-05 02:51:36,443: Train batch 1180: loss: 11.14 grad norm: 34.38 time: 1.426
2025-12-05 02:52:04,234: Train batch 1200: loss: 7.96 grad norm: 25.54 time: 1.429
2025-12-05 02:52:35,005: Train batch 1220: loss: 10.32 grad norm: 43.33 time: 1.391
2025-12-05 02:53:05,141: Train batch 1240: loss: 12.69 grad norm: 32.05 time: 1.634
2025-12-05 02:53:33,216: Train batch 1260: loss: 7.99 grad norm: 28.77 time: 1.436
2025-12-05 02:54:02,115: Train batch 1280: loss: 11.30 grad norm: 35.50 time: 1.461
2025-12-05 02:54:30,467: Train batch 1300: loss: 7.80 grad norm: 31.25 time: 1.306
2025-12-05 02:55:00,126: Train batch 1320: loss: 8.16 grad norm: 31.59 time: 1.426
2025-12-05 02:55:28,859: Train batch 1340: loss: 8.40 grad norm: 30.86 time: 1.476
2025-12-05 02:55:56,855: Train batch 1360: loss: 11.35 grad norm: 37.66 time: 1.819
2025-12-05 02:56:25,218: Train batch 1380: loss: 7.50 grad norm: 31.17 time: 1.211
2025-12-05 02:56:54,179: Train batch 1400: loss: 7.93 grad norm: 29.17 time: 1.821
2025-12-05 02:57:21,863: Train batch 1420: loss: 6.38 grad norm: 25.77 time: 1.523
2025-12-05 02:57:51,566: Train batch 1440: loss: 5.92 grad norm: 29.63 time: 1.677
2025-12-05 02:58:20,989: Train batch 1460: loss: 8.09 grad norm: 32.09 time: 1.256
2025-12-05 02:58:49,798: Train batch 1480: loss: 7.59 grad norm: 29.80 time: 1.625
2025-12-05 02:59:19,654: Train batch 1500: loss: 9.18 grad norm: 34.78 time: 1.363
2025-12-05 02:59:19,656: Running test after training batch: 1500
2025-12-05 02:59:26,551: Val batch 1500: PER (avg): 0.1233 CTC Loss (avg): 12.1110 time: 6.893
2025-12-05 02:59:26,551: t15.2023.08.13 val PER: 0.1528
2025-12-05 02:59:26,551: t15.2023.08.18 val PER: 0.1090
2025-12-05 02:59:26,551: t15.2023.08.20 val PER: 0.1152
2025-12-05 02:59:26,552: t15.2023.08.25 val PER: 0.1069
2025-12-05 02:59:26,552: t15.2023.08.27 val PER: 0.1688
2025-12-05 02:59:26,552: t15.2023.09.01 val PER: 0.0828
2025-12-05 02:59:26,552: t15.2023.09.03 val PER: 0.1615
2025-12-05 02:59:26,552: t15.2023.09.24 val PER: 0.1456
2025-12-05 02:59:26,552: t15.2023.09.29 val PER: 0.1222
2025-12-05 02:59:26,553: t15.2023.10.01 val PER: 0.1270
2025-12-05 02:59:26,553: t15.2023.10.06 val PER: 0.1001
2025-12-05 02:59:26,553: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 02:59:26,553: t15.2023.10.13 val PER: 0.1161
2025-12-05 02:59:26,553: t15.2023.10.15 val PER: 0.1303
2025-12-05 02:59:26,553: New best test PER 0.1430 --> 0.1233
2025-12-05 02:59:26,553: Checkpointing model
2025-12-05 02:59:30,252: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 02:59:58,594: Train batch 1520: loss: 6.93 grad norm: 28.70 time: 1.813
2025-12-05 03:00:28,716: Train batch 1540: loss: 9.67 grad norm: 33.26 time: 1.495
2025-12-05 03:00:56,894: Train batch 1560: loss: 7.01 grad norm: 32.48 time: 1.431
2025-12-05 03:01:24,569: Train batch 1580: loss: 7.66 grad norm: 31.37 time: 1.467
2025-12-05 03:01:53,074: Train batch 1600: loss: 6.50 grad norm: 30.51 time: 1.302
2025-12-05 03:02:19,864: Train batch 1620: loss: 5.78 grad norm: 26.27 time: 1.484
2025-12-05 03:02:48,269: Train batch 1640: loss: 5.68 grad norm: 26.34 time: 1.567
2025-12-05 03:03:17,441: Train batch 1660: loss: 6.67 grad norm: 33.10 time: 1.373
2025-12-05 03:03:47,102: Train batch 1680: loss: 7.14 grad norm: 31.29 time: 1.424
2025-12-05 03:04:16,705: Train batch 1700: loss: 4.41 grad norm: 22.97 time: 1.315
2025-12-05 03:04:47,193: Train batch 1720: loss: 5.50 grad norm: 27.06 time: 1.284
2025-12-05 03:05:16,790: Train batch 1740: loss: 6.86 grad norm: 33.93 time: 1.549
2025-12-05 03:05:46,359: Train batch 1760: loss: 7.40 grad norm: 30.80 time: 1.722
2025-12-05 03:06:14,364: Train batch 1780: loss: 7.64 grad norm: 34.13 time: 1.295
2025-12-05 03:06:42,570: Train batch 1800: loss: 5.11 grad norm: 25.87 time: 1.493
2025-12-05 03:07:12,042: Train batch 1820: loss: 3.75 grad norm: 24.02 time: 1.903
2025-12-05 03:07:42,192: Train batch 1840: loss: 4.93 grad norm: 26.69 time: 1.279
2025-12-05 03:08:11,828: Train batch 1860: loss: 5.41 grad norm: 29.55 time: 1.224
2025-12-05 03:08:39,757: Train batch 1880: loss: 5.45 grad norm: 27.56 time: 1.339
2025-12-05 03:09:07,884: Train batch 1900: loss: 6.92 grad norm: 29.62 time: 1.576
2025-12-05 03:09:34,975: Train batch 1920: loss: 5.62 grad norm: 29.42 time: 1.181
2025-12-05 03:10:03,397: Train batch 1940: loss: 6.59 grad norm: 29.25 time: 1.543
2025-12-05 03:10:30,970: Train batch 1960: loss: 3.17 grad norm: 20.42 time: 1.289
2025-12-05 03:10:59,778: Train batch 1980: loss: 7.27 grad norm: 37.14 time: 1.667
2025-12-05 03:11:27,798: Train batch 2000: loss: 5.20 grad norm: 27.35 time: 1.346
2025-12-05 03:11:27,801: Running test after training batch: 2000
2025-12-05 03:11:34,712: Val batch 2000: PER (avg): 0.1117 CTC Loss (avg): 11.7129 time: 6.910
2025-12-05 03:11:34,712: t15.2023.08.13 val PER: 0.1351
2025-12-05 03:11:34,712: t15.2023.08.18 val PER: 0.1048
2025-12-05 03:11:34,712: t15.2023.08.20 val PER: 0.1009
2025-12-05 03:11:34,713: t15.2023.08.25 val PER: 0.0858
2025-12-05 03:11:34,713: t15.2023.08.27 val PER: 0.1511
2025-12-05 03:11:34,713: t15.2023.09.01 val PER: 0.0787
2025-12-05 03:11:34,713: t15.2023.09.03 val PER: 0.1520
2025-12-05 03:11:34,713: t15.2023.09.24 val PER: 0.1153
2025-12-05 03:11:34,713: t15.2023.09.29 val PER: 0.1174
2025-12-05 03:11:34,714: t15.2023.10.01 val PER: 0.1189
2025-12-05 03:11:34,714: t15.2023.10.06 val PER: 0.1023
2025-12-05 03:11:34,714: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 03:11:34,714: t15.2023.10.13 val PER: 0.1145
2025-12-05 03:11:34,714: t15.2023.10.15 val PER: 0.1040
2025-12-05 03:11:34,714: New best test PER 0.1233 --> 0.1117
2025-12-05 03:11:34,714: Checkpointing model
2025-12-05 03:11:38,331: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 03:12:07,140: Train batch 2020: loss: 3.52 grad norm: 22.96 time: 1.482
2025-12-05 03:12:36,264: Train batch 2040: loss: 4.51 grad norm: 25.91 time: 1.263
2025-12-05 03:13:06,506: Train batch 2060: loss: 7.22 grad norm: 36.52 time: 1.702
2025-12-05 03:13:36,170: Train batch 2080: loss: 5.56 grad norm: 30.75 time: 1.279
2025-12-05 03:14:03,707: Train batch 2100: loss: 4.06 grad norm: 25.29 time: 1.617
2025-12-05 03:14:33,648: Train batch 2120: loss: 4.39 grad norm: 27.49 time: 1.578
2025-12-05 03:15:04,418: Train batch 2140: loss: 4.41 grad norm: 25.69 time: 1.823
2025-12-05 03:15:33,867: Train batch 2160: loss: 3.82 grad norm: 25.62 time: 1.442
2025-12-05 03:16:04,256: Train batch 2180: loss: 4.28 grad norm: 25.95 time: 1.163
2025-12-05 03:16:33,751: Train batch 2200: loss: 3.33 grad norm: 25.09 time: 1.367
2025-12-05 03:17:04,389: Train batch 2220: loss: 3.72 grad norm: 26.14 time: 1.619
2025-12-05 03:17:32,887: Train batch 2240: loss: 2.62 grad norm: 19.53 time: 1.523
2025-12-05 03:18:00,539: Train batch 2260: loss: 3.37 grad norm: 36.64 time: 1.510
2025-12-05 03:18:29,460: Train batch 2280: loss: 4.76 grad norm: 30.98 time: 1.258
2025-12-05 03:18:58,948: Train batch 2300: loss: 4.32 grad norm: 30.30 time: 1.702
2025-12-05 03:19:28,193: Train batch 2320: loss: 4.59 grad norm: 28.85 time: 1.311
2025-12-05 03:19:56,793: Train batch 2340: loss: 4.26 grad norm: 29.30 time: 1.841
2025-12-05 03:20:26,931: Train batch 2360: loss: 4.43 grad norm: 25.88 time: 1.210
2025-12-05 03:20:55,907: Train batch 2380: loss: 2.68 grad norm: 18.56 time: 1.362
2025-12-05 03:21:25,200: Train batch 2400: loss: 3.67 grad norm: 34.05 time: 1.280
2025-12-05 03:21:53,754: Train batch 2420: loss: 5.89 grad norm: 26.97 time: 1.388
2025-12-05 03:22:22,671: Train batch 2440: loss: 4.51 grad norm: 26.33 time: 1.421
2025-12-05 03:22:51,032: Train batch 2460: loss: 2.15 grad norm: 17.15 time: 1.241
2025-12-05 03:23:21,989: Train batch 2480: loss: 3.01 grad norm: 19.74 time: 1.520
2025-12-05 03:23:52,210: Train batch 2500: loss: 4.72 grad norm: 26.39 time: 1.520
2025-12-05 03:23:52,212: Running test after training batch: 2500
2025-12-05 03:23:59,133: Val batch 2500: PER (avg): 0.1056 CTC Loss (avg): 11.7042 time: 6.921
2025-12-05 03:23:59,134: t15.2023.08.13 val PER: 0.1310
2025-12-05 03:23:59,134: t15.2023.08.18 val PER: 0.1081
2025-12-05 03:23:59,134: t15.2023.08.20 val PER: 0.0977
2025-12-05 03:23:59,134: t15.2023.08.25 val PER: 0.0904
2025-12-05 03:23:59,135: t15.2023.08.27 val PER: 0.1415
2025-12-05 03:23:59,135: t15.2023.09.01 val PER: 0.0795
2025-12-05 03:23:59,135: t15.2023.09.03 val PER: 0.1461
2025-12-05 03:23:59,135: t15.2023.09.24 val PER: 0.1032
2025-12-05 03:23:59,135: t15.2023.09.29 val PER: 0.1013
2025-12-05 03:23:59,136: t15.2023.10.01 val PER: 0.0977
2025-12-05 03:23:59,136: t15.2023.10.06 val PER: 0.0936
2025-12-05 03:23:59,136: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 03:23:59,136: t15.2023.10.13 val PER: 0.1032
2025-12-05 03:23:59,136: t15.2023.10.15 val PER: 0.0922
2025-12-05 03:23:59,136: New best test PER 0.1117 --> 0.1056
2025-12-05 03:23:59,136: Checkpointing model
2025-12-05 03:24:02,824: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 03:24:31,046: Train batch 2520: loss: 2.77 grad norm: 20.57 time: 1.399
2025-12-05 03:24:59,369: Train batch 2540: loss: 3.39 grad norm: 27.45 time: 1.354
2025-12-05 03:25:29,327: Train batch 2560: loss: 2.98 grad norm: 23.99 time: 1.595
2025-12-05 03:25:58,127: Train batch 2580: loss: 3.67 grad norm: 25.72 time: 1.291
2025-12-05 03:26:27,892: Train batch 2600: loss: 2.84 grad norm: 23.46 time: 1.617
2025-12-05 03:26:58,383: Train batch 2620: loss: 4.33 grad norm: 28.71 time: 1.299
2025-12-05 03:27:27,216: Train batch 2640: loss: 3.01 grad norm: 23.77 time: 1.621
2025-12-05 03:27:54,861: Train batch 2660: loss: 4.34 grad norm: 29.10 time: 1.337
2025-12-05 03:28:24,597: Train batch 2680: loss: 3.26 grad norm: 24.42 time: 1.311
2025-12-05 03:28:51,656: Train batch 2700: loss: 2.93 grad norm: 25.33 time: 1.358
2025-12-05 03:29:21,902: Train batch 2720: loss: 3.97 grad norm: 27.57 time: 1.399
2025-12-05 03:29:50,426: Train batch 2740: loss: 4.73 grad norm: 30.87 time: 1.426
2025-12-05 03:30:18,901: Train batch 2760: loss: 3.87 grad norm: 30.11 time: 1.689
2025-12-05 03:30:48,743: Train batch 2780: loss: 2.30 grad norm: 20.36 time: 1.375
2025-12-05 03:31:16,643: Train batch 2800: loss: 3.42 grad norm: 28.42 time: 1.251
2025-12-05 03:31:46,346: Train batch 2820: loss: 2.23 grad norm: 20.90 time: 1.521
2025-12-05 03:32:14,429: Train batch 2840: loss: 2.18 grad norm: 20.56 time: 1.616
2025-12-05 03:32:42,373: Train batch 2860: loss: 3.89 grad norm: 26.54 time: 1.372
2025-12-05 03:33:11,146: Train batch 2880: loss: 2.75 grad norm: 26.28 time: 1.322
2025-12-05 03:33:40,470: Train batch 2900: loss: 3.10 grad norm: 26.29 time: 1.367
2025-12-05 03:34:09,329: Train batch 2920: loss: 2.88 grad norm: 23.34 time: 1.519
2025-12-05 03:34:38,247: Train batch 2940: loss: 2.40 grad norm: 21.97 time: 1.275
2025-12-05 03:35:07,349: Train batch 2960: loss: 3.13 grad norm: 22.00 time: 1.328
2025-12-05 03:35:35,769: Train batch 2980: loss: 2.09 grad norm: 22.91 time: 1.285
2025-12-05 03:36:04,906: Train batch 3000: loss: 1.91 grad norm: 18.79 time: 1.285
2025-12-05 03:36:04,909: Running test after training batch: 3000
2025-12-05 03:36:11,835: Val batch 3000: PER (avg): 0.1029 CTC Loss (avg): 11.6633 time: 6.926
2025-12-05 03:36:11,836: t15.2023.08.13 val PER: 0.1247
2025-12-05 03:36:11,836: t15.2023.08.18 val PER: 0.0947
2025-12-05 03:36:11,836: t15.2023.08.20 val PER: 0.0969
2025-12-05 03:36:11,836: t15.2023.08.25 val PER: 0.0723
2025-12-05 03:36:11,836: t15.2023.08.27 val PER: 0.1399
2025-12-05 03:36:11,836: t15.2023.09.01 val PER: 0.0698
2025-12-05 03:36:11,837: t15.2023.09.03 val PER: 0.1425
2025-12-05 03:36:11,837: t15.2023.09.24 val PER: 0.1129
2025-12-05 03:36:11,837: t15.2023.09.29 val PER: 0.0981
2025-12-05 03:36:11,837: t15.2023.10.01 val PER: 0.1042
2025-12-05 03:36:11,837: t15.2023.10.06 val PER: 0.0980
2025-12-05 03:36:11,837: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 03:36:11,838: t15.2023.10.13 val PER: 0.1016
2025-12-05 03:36:11,838: t15.2023.10.15 val PER: 0.1040
2025-12-05 03:36:11,838: New best test PER 0.1056 --> 0.1029
2025-12-05 03:36:11,838: Checkpointing model
2025-12-05 03:36:15,361: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 03:36:44,157: Train batch 3020: loss: 2.14 grad norm: 21.87 time: 1.253
2025-12-05 03:37:14,160: Train batch 3040: loss: 2.70 grad norm: 22.39 time: 1.722
2025-12-05 03:37:42,011: Train batch 3060: loss: 2.32 grad norm: 25.54 time: 1.322
2025-12-05 03:38:11,585: Train batch 3080: loss: 2.43 grad norm: 22.05 time: 1.367
2025-12-05 03:38:39,434: Train batch 3100: loss: 2.31 grad norm: 22.44 time: 1.882
2025-12-05 03:39:07,169: Train batch 3120: loss: 3.22 grad norm: 24.71 time: 1.336
2025-12-05 03:39:35,951: Train batch 3140: loss: 2.90 grad norm: 22.73 time: 1.603
2025-12-05 03:40:05,029: Train batch 3160: loss: 1.97 grad norm: 20.20 time: 1.229
2025-12-05 03:40:34,773: Train batch 3180: loss: 3.23 grad norm: 28.75 time: 1.598
2025-12-05 03:41:03,255: Train batch 3200: loss: 3.59 grad norm: 28.25 time: 1.656
2025-12-05 03:41:31,680: Train batch 3220: loss: 2.66 grad norm: 24.77 time: 1.349
2025-12-05 03:42:01,156: Train batch 3240: loss: 3.05 grad norm: 30.02 time: 1.529
2025-12-05 03:42:29,223: Train batch 3260: loss: 3.02 grad norm: 27.27 time: 1.420
2025-12-05 03:42:58,420: Train batch 3280: loss: 2.53 grad norm: 22.07 time: 1.400
2025-12-05 03:43:27,890: Train batch 3300: loss: 1.81 grad norm: 18.18 time: 1.627
2025-12-05 03:43:57,966: Train batch 3320: loss: 2.49 grad norm: 23.95 time: 1.434
2025-12-05 03:44:27,241: Train batch 3340: loss: 4.45 grad norm: 24.54 time: 1.422
2025-12-05 03:44:56,482: Train batch 3360: loss: 2.12 grad norm: 20.96 time: 1.649
2025-12-05 03:45:26,039: Train batch 3380: loss: 2.66 grad norm: 24.91 time: 1.518
2025-12-05 03:45:56,901: Train batch 3400: loss: 3.08 grad norm: 26.10 time: 1.704
2025-12-05 03:46:25,664: Train batch 3420: loss: 2.82 grad norm: 26.18 time: 1.588
2025-12-05 03:46:55,161: Train batch 3440: loss: 2.25 grad norm: 22.47 time: 1.326
2025-12-05 03:47:25,509: Train batch 3460: loss: 2.09 grad norm: 21.39 time: 1.663
2025-12-05 03:47:55,069: Train batch 3480: loss: 1.69 grad norm: 19.87 time: 1.476
2025-12-05 03:48:23,588: Train batch 3500: loss: 1.71 grad norm: 23.69 time: 1.577
2025-12-05 03:48:23,591: Running test after training batch: 3500
2025-12-05 03:48:30,510: Val batch 3500: PER (avg): 0.1011 CTC Loss (avg): 12.2709 time: 6.918
2025-12-05 03:48:30,510: t15.2023.08.13 val PER: 0.1237
2025-12-05 03:48:30,511: t15.2023.08.18 val PER: 0.0956
2025-12-05 03:48:30,511: t15.2023.08.20 val PER: 0.0874
2025-12-05 03:48:30,511: t15.2023.08.25 val PER: 0.0813
2025-12-05 03:48:30,511: t15.2023.08.27 val PER: 0.1350
2025-12-05 03:48:30,511: t15.2023.09.01 val PER: 0.0739
2025-12-05 03:48:30,511: t15.2023.09.03 val PER: 0.1461
2025-12-05 03:48:30,512: t15.2023.09.24 val PER: 0.1129
2025-12-05 03:48:30,512: t15.2023.09.29 val PER: 0.0868
2025-12-05 03:48:30,512: t15.2023.10.01 val PER: 0.1107
2025-12-05 03:48:30,512: t15.2023.10.06 val PER: 0.0893
2025-12-05 03:48:30,512: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 03:48:30,512: t15.2023.10.13 val PER: 0.1032
2025-12-05 03:48:30,512: t15.2023.10.15 val PER: 0.0908
2025-12-05 03:48:30,512: New best test PER 0.1029 --> 0.1011
2025-12-05 03:48:30,513: Checkpointing model
2025-12-05 03:48:34,208: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 03:49:03,944: Train batch 3520: loss: 2.51 grad norm: 25.08 time: 1.394
2025-12-05 03:49:33,040: Train batch 3540: loss: 2.05 grad norm: 24.42 time: 1.328
2025-12-05 03:50:02,565: Train batch 3560: loss: 3.81 grad norm: 23.50 time: 1.586
2025-12-05 03:50:30,976: Train batch 3580: loss: 2.38 grad norm: 24.01 time: 1.389
2025-12-05 03:50:58,451: Train batch 3600: loss: 1.93 grad norm: 21.76 time: 1.556
2025-12-05 03:51:29,891: Train batch 3620: loss: 1.82 grad norm: 22.40 time: 1.903
2025-12-05 03:51:58,436: Train batch 3640: loss: 2.50 grad norm: 24.89 time: 1.304
2025-12-05 03:52:27,311: Train batch 3660: loss: 2.08 grad norm: 21.91 time: 1.424
2025-12-05 03:52:57,182: Train batch 3680: loss: 2.44 grad norm: 23.51 time: 1.255
2025-12-05 03:53:25,525: Train batch 3700: loss: 2.06 grad norm: 22.04 time: 1.487
2025-12-05 03:53:54,480: Train batch 3720: loss: 1.54 grad norm: 21.80 time: 1.260
2025-12-05 03:54:21,785: Train batch 3740: loss: 2.30 grad norm: 29.99 time: 1.188
2025-12-05 03:54:50,667: Train batch 3760: loss: 2.23 grad norm: 22.85 time: 1.387
2025-12-05 03:55:20,604: Train batch 3780: loss: 1.96 grad norm: 20.38 time: 1.573
2025-12-05 03:55:49,190: Train batch 3800: loss: 3.25 grad norm: 26.24 time: 1.535
2025-12-05 03:56:17,884: Train batch 3820: loss: 1.79 grad norm: 19.28 time: 1.725
2025-12-05 03:56:46,348: Train batch 3840: loss: 2.77 grad norm: 25.55 time: 1.517
2025-12-05 03:57:15,207: Train batch 3860: loss: 1.78 grad norm: 22.14 time: 1.292
2025-12-05 03:57:44,163: Train batch 3880: loss: 2.68 grad norm: 22.12 time: 1.580
2025-12-05 03:58:12,675: Train batch 3900: loss: 1.94 grad norm: 22.11 time: 1.516
2025-12-05 03:58:42,397: Train batch 3920: loss: 1.54 grad norm: 24.94 time: 1.350
2025-12-05 03:59:11,908: Train batch 3940: loss: 2.04 grad norm: 24.86 time: 1.569
2025-12-05 03:59:40,702: Train batch 3960: loss: 1.54 grad norm: 19.98 time: 1.270
2025-12-05 04:00:12,047: Train batch 3980: loss: 1.79 grad norm: 20.24 time: 1.591
2025-12-05 04:00:41,143: Train batch 4000: loss: 0.93 grad norm: 14.00 time: 1.256
2025-12-05 04:00:41,146: Running test after training batch: 4000
2025-12-05 04:00:48,093: Val batch 4000: PER (avg): 0.1014 CTC Loss (avg): 12.2641 time: 6.946
2025-12-05 04:00:48,093: t15.2023.08.13 val PER: 0.1331
2025-12-05 04:00:48,094: t15.2023.08.18 val PER: 0.0972
2025-12-05 04:00:48,094: t15.2023.08.20 val PER: 0.0882
2025-12-05 04:00:48,094: t15.2023.08.25 val PER: 0.0738
2025-12-05 04:00:48,094: t15.2023.08.27 val PER: 0.1495
2025-12-05 04:00:48,094: t15.2023.09.01 val PER: 0.0666
2025-12-05 04:00:48,094: t15.2023.09.03 val PER: 0.1437
2025-12-05 04:00:48,095: t15.2023.09.24 val PER: 0.1189
2025-12-05 04:00:48,095: t15.2023.09.29 val PER: 0.0965
2025-12-05 04:00:48,095: t15.2023.10.01 val PER: 0.0993
2025-12-05 04:00:48,095: t15.2023.10.06 val PER: 0.0829
2025-12-05 04:00:48,095: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 04:00:48,095: t15.2023.10.13 val PER: 0.1016
2025-12-05 04:00:48,095: t15.2023.10.15 val PER: 0.0922
2025-12-05 04:01:18,190: Train batch 4020: loss: 1.50 grad norm: 22.94 time: 1.836
2025-12-05 04:01:47,552: Train batch 4040: loss: 2.22 grad norm: 21.42 time: 1.294
2025-12-05 04:02:16,290: Train batch 4060: loss: 2.47 grad norm: 25.22 time: 1.624
2025-12-05 04:02:46,943: Train batch 4080: loss: 2.56 grad norm: 24.13 time: 1.400
2025-12-05 04:03:16,511: Train batch 4100: loss: 1.94 grad norm: 22.11 time: 1.230
2025-12-05 04:03:45,607: Train batch 4120: loss: 2.43 grad norm: 23.38 time: 1.689
2025-12-05 04:04:13,437: Train batch 4140: loss: 1.41 grad norm: 19.32 time: 1.618
2025-12-05 04:04:40,658: Train batch 4160: loss: 1.75 grad norm: 19.13 time: 1.622
2025-12-05 04:05:10,261: Train batch 4180: loss: 1.49 grad norm: 20.86 time: 1.359
2025-12-05 04:05:39,159: Train batch 4200: loss: 1.61 grad norm: 20.47 time: 1.293
2025-12-05 04:06:07,352: Train batch 4220: loss: 2.26 grad norm: 23.74 time: 1.644
2025-12-05 04:06:35,536: Train batch 4240: loss: 2.04 grad norm: 16.85 time: 1.506
2025-12-05 04:07:04,167: Train batch 4260: loss: 1.58 grad norm: 19.74 time: 1.216
2025-12-05 04:07:32,849: Train batch 4280: loss: 1.26 grad norm: 18.75 time: 1.359
2025-12-05 04:08:02,580: Train batch 4300: loss: 1.17 grad norm: 18.10 time: 1.727
2025-12-05 04:08:32,842: Train batch 4320: loss: 2.41 grad norm: 24.52 time: 1.514
2025-12-05 04:09:01,251: Train batch 4340: loss: 1.62 grad norm: 18.19 time: 1.399
2025-12-05 04:09:31,259: Train batch 4360: loss: 1.35 grad norm: 19.26 time: 1.822
2025-12-05 04:10:00,045: Train batch 4380: loss: 1.83 grad norm: 19.92 time: 1.506
2025-12-05 04:10:29,456: Train batch 4400: loss: 1.29 grad norm: 16.48 time: 1.384
2025-12-05 04:10:59,264: Train batch 4420: loss: 2.26 grad norm: 26.22 time: 1.923
2025-12-05 04:11:26,415: Train batch 4440: loss: 1.15 grad norm: 18.27 time: 1.207
2025-12-05 04:11:55,977: Train batch 4460: loss: 1.47 grad norm: 18.78 time: 1.306
2025-12-05 04:12:25,212: Train batch 4480: loss: 1.67 grad norm: 20.67 time: 1.429
2025-12-05 04:12:54,886: Train batch 4500: loss: 1.32 grad norm: 19.30 time: 1.538
2025-12-05 04:12:54,889: Running test after training batch: 4500
2025-12-05 04:13:02,053: Val batch 4500: PER (avg): 0.1004 CTC Loss (avg): 12.3265 time: 7.162
2025-12-05 04:13:02,053: t15.2023.08.13 val PER: 0.1258
2025-12-05 04:13:02,054: t15.2023.08.18 val PER: 0.0989
2025-12-05 04:13:02,054: t15.2023.08.20 val PER: 0.0905
2025-12-05 04:13:02,054: t15.2023.08.25 val PER: 0.0738
2025-12-05 04:13:02,054: t15.2023.08.27 val PER: 0.1399
2025-12-05 04:13:02,054: t15.2023.09.01 val PER: 0.0698
2025-12-05 04:13:02,054: t15.2023.09.03 val PER: 0.1449
2025-12-05 04:13:02,055: t15.2023.09.24 val PER: 0.1068
2025-12-05 04:13:02,055: t15.2023.09.29 val PER: 0.0949
2025-12-05 04:13:02,055: t15.2023.10.01 val PER: 0.1059
2025-12-05 04:13:02,055: t15.2023.10.06 val PER: 0.0840
2025-12-05 04:13:02,055: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 04:13:02,056: t15.2023.10.13 val PER: 0.1000
2025-12-05 04:13:02,056: t15.2023.10.15 val PER: 0.0908
2025-12-05 04:13:02,056: New best test PER 0.1011 --> 0.1004
2025-12-05 04:13:02,056: Checkpointing model
2025-12-05 04:13:05,736: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 04:13:35,498: Train batch 4520: loss: 0.92 grad norm: 12.03 time: 1.495
2025-12-05 04:14:04,314: Train batch 4540: loss: 1.97 grad norm: 21.51 time: 1.439
2025-12-05 04:14:32,318: Train batch 4560: loss: 1.57 grad norm: 25.31 time: 1.663
2025-12-05 04:15:00,111: Train batch 4580: loss: 1.39 grad norm: 18.63 time: 1.262
2025-12-05 04:15:28,493: Train batch 4600: loss: 1.27 grad norm: 13.58 time: 1.323
2025-12-05 04:15:58,263: Train batch 4620: loss: 1.11 grad norm: 14.13 time: 1.250
2025-12-05 04:16:27,609: Train batch 4640: loss: 1.78 grad norm: 22.10 time: 1.334
2025-12-05 04:16:56,375: Train batch 4660: loss: 0.90 grad norm: 13.14 time: 1.518
2025-12-05 04:17:23,672: Train batch 4680: loss: 1.36 grad norm: 18.28 time: 1.487
2025-12-05 04:17:53,287: Train batch 4700: loss: 1.38 grad norm: 18.44 time: 1.560
2025-12-05 04:18:21,392: Train batch 4720: loss: 1.51 grad norm: 22.09 time: 1.377
2025-12-05 04:18:49,661: Train batch 4740: loss: 1.32 grad norm: 16.19 time: 1.360
2025-12-05 04:19:18,876: Train batch 4760: loss: 2.35 grad norm: 23.72 time: 1.660
2025-12-05 04:19:48,641: Train batch 4780: loss: 1.60 grad norm: 19.07 time: 1.698
2025-12-05 04:20:17,531: Train batch 4800: loss: 2.12 grad norm: 22.15 time: 1.443
2025-12-05 04:20:47,275: Train batch 4820: loss: 1.56 grad norm: 18.45 time: 1.387
2025-12-05 04:21:17,102: Train batch 4840: loss: 2.07 grad norm: 21.32 time: 1.551
2025-12-05 04:21:47,085: Train batch 4860: loss: 2.00 grad norm: 22.82 time: 1.678
2025-12-05 04:22:15,915: Train batch 4880: loss: 1.60 grad norm: 18.90 time: 1.440
2025-12-05 04:22:44,884: Train batch 4900: loss: 1.40 grad norm: 23.87 time: 1.584
2025-12-05 04:23:14,711: Train batch 4920: loss: 1.32 grad norm: 16.64 time: 1.393
2025-12-05 04:23:42,419: Train batch 4940: loss: 1.35 grad norm: 18.84 time: 1.263
2025-12-05 04:24:12,388: Train batch 4960: loss: 1.05 grad norm: 19.13 time: 1.850
2025-12-05 04:24:41,901: Train batch 4980: loss: 1.53 grad norm: 17.38 time: 1.404
2025-12-05 04:25:11,170: Train batch 5000: loss: 1.92 grad norm: 21.88 time: 1.365
2025-12-05 04:25:11,173: Running test after training batch: 5000
2025-12-05 04:25:18,071: Val batch 5000: PER (avg): 0.0992 CTC Loss (avg): 12.4589 time: 6.897
2025-12-05 04:25:18,071: t15.2023.08.13 val PER: 0.1216
2025-12-05 04:25:18,072: t15.2023.08.18 val PER: 0.1014
2025-12-05 04:25:18,072: t15.2023.08.20 val PER: 0.0905
2025-12-05 04:25:18,072: t15.2023.08.25 val PER: 0.0783
2025-12-05 04:25:18,072: t15.2023.08.27 val PER: 0.1383
2025-12-05 04:25:18,073: t15.2023.09.01 val PER: 0.0657
2025-12-05 04:25:18,073: t15.2023.09.03 val PER: 0.1413
2025-12-05 04:25:18,073: t15.2023.09.24 val PER: 0.1080
2025-12-05 04:25:18,073: t15.2023.09.29 val PER: 0.0949
2025-12-05 04:25:18,073: t15.2023.10.01 val PER: 0.0977
2025-12-05 04:25:18,073: t15.2023.10.06 val PER: 0.0807
2025-12-05 04:25:18,074: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 04:25:18,074: t15.2023.10.13 val PER: 0.1000
2025-12-05 04:25:18,074: t15.2023.10.15 val PER: 0.0922
2025-12-05 04:25:18,074: New best test PER 0.1004 --> 0.0992
2025-12-05 04:25:18,074: Checkpointing model
2025-12-05 04:25:21,749: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 04:25:51,017: Train batch 5020: loss: 1.59 grad norm: 21.74 time: 1.380
2025-12-05 04:26:20,143: Train batch 5040: loss: 1.21 grad norm: 17.14 time: 1.299
2025-12-05 04:26:48,451: Train batch 5060: loss: 1.44 grad norm: 17.49 time: 1.211
2025-12-05 04:27:16,774: Train batch 5080: loss: 1.57 grad norm: 19.38 time: 1.130
2025-12-05 04:27:45,756: Train batch 5100: loss: 1.77 grad norm: 20.97 time: 1.265
2025-12-05 04:28:14,075: Train batch 5120: loss: 1.86 grad norm: 21.29 time: 1.547
2025-12-05 04:28:41,005: Train batch 5140: loss: 1.32 grad norm: 19.22 time: 1.290
2025-12-05 04:29:09,810: Train batch 5160: loss: 1.43 grad norm: 18.17 time: 1.362
2025-12-05 04:29:38,682: Train batch 5180: loss: 1.78 grad norm: 22.68 time: 1.575
2025-12-05 04:30:06,784: Train batch 5200: loss: 1.58 grad norm: 19.65 time: 1.682
2025-12-05 04:30:35,595: Train batch 5220: loss: 1.27 grad norm: 17.54 time: 1.370
2025-12-05 04:31:05,812: Train batch 5240: loss: 1.35 grad norm: 19.81 time: 1.528
2025-12-05 04:31:33,457: Train batch 5260: loss: 1.57 grad norm: 22.42 time: 1.474
2025-12-05 04:32:01,420: Train batch 5280: loss: 1.53 grad norm: 19.72 time: 1.187
2025-12-05 04:32:29,896: Train batch 5300: loss: 1.42 grad norm: 19.88 time: 1.382
2025-12-05 04:32:58,719: Train batch 5320: loss: 1.65 grad norm: 19.93 time: 1.308
2025-12-05 04:33:26,013: Train batch 5340: loss: 1.52 grad norm: 18.87 time: 1.264
2025-12-05 04:33:56,017: Train batch 5360: loss: 2.44 grad norm: 28.15 time: 1.708
2025-12-05 04:34:24,483: Train batch 5380: loss: 1.71 grad norm: 22.49 time: 1.371
2025-12-05 04:34:53,975: Train batch 5400: loss: 1.71 grad norm: 24.22 time: 1.261
2025-12-05 04:35:23,938: Train batch 5420: loss: 1.39 grad norm: 25.98 time: 1.456
2025-12-05 04:35:52,292: Train batch 5440: loss: 1.43 grad norm: 19.21 time: 1.304
2025-12-05 04:36:20,884: Train batch 5460: loss: 1.09 grad norm: 14.99 time: 1.160
2025-12-05 04:36:49,313: Train batch 5480: loss: 1.32 grad norm: 19.36 time: 1.276
2025-12-05 04:37:17,997: Train batch 5500: loss: 1.69 grad norm: 22.91 time: 1.350
2025-12-05 04:37:17,999: Running test after training batch: 5500
2025-12-05 04:37:24,962: Val batch 5500: PER (avg): 0.0981 CTC Loss (avg): 12.4062 time: 6.962
2025-12-05 04:37:24,962: t15.2023.08.13 val PER: 0.1258
2025-12-05 04:37:24,962: t15.2023.08.18 val PER: 0.0872
2025-12-05 04:37:24,963: t15.2023.08.20 val PER: 0.0905
2025-12-05 04:37:24,963: t15.2023.08.25 val PER: 0.0693
2025-12-05 04:37:24,963: t15.2023.08.27 val PER: 0.1367
2025-12-05 04:37:24,963: t15.2023.09.01 val PER: 0.0698
2025-12-05 04:37:24,963: t15.2023.09.03 val PER: 0.1437
2025-12-05 04:37:24,964: t15.2023.09.24 val PER: 0.1044
2025-12-05 04:37:24,964: t15.2023.09.29 val PER: 0.0965
2025-12-05 04:37:24,964: t15.2023.10.01 val PER: 0.1010
2025-12-05 04:37:24,964: t15.2023.10.06 val PER: 0.0861
2025-12-05 04:37:24,964: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 04:37:24,965: t15.2023.10.13 val PER: 0.1000
2025-12-05 04:37:24,965: t15.2023.10.15 val PER: 0.0864
2025-12-05 04:37:24,965: New best test PER 0.0992 --> 0.0981
2025-12-05 04:37:24,965: Checkpointing model
2025-12-05 04:37:28,644: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 04:38:00,282: Train batch 5520: loss: 2.38 grad norm: 22.07 time: 1.440
2025-12-05 04:38:29,293: Train batch 5540: loss: 1.62 grad norm: 21.62 time: 1.904
2025-12-05 04:38:58,099: Train batch 5560: loss: 1.93 grad norm: 22.01 time: 1.441
2025-12-05 04:39:27,101: Train batch 5580: loss: 1.40 grad norm: 18.51 time: 1.555
2025-12-05 04:39:55,349: Train batch 5600: loss: 1.22 grad norm: 17.44 time: 1.270
2025-12-05 04:40:24,238: Train batch 5620: loss: 1.08 grad norm: 16.61 time: 1.496
2025-12-05 04:40:52,415: Train batch 5640: loss: 1.76 grad norm: 27.25 time: 1.357
2025-12-05 04:41:21,302: Train batch 5660: loss: 0.85 grad norm: 14.01 time: 1.448
2025-12-05 04:41:50,224: Train batch 5680: loss: 1.41 grad norm: 21.52 time: 1.379
2025-12-05 04:42:21,089: Train batch 5700: loss: 1.42 grad norm: 19.81 time: 1.251
2025-12-05 04:42:50,220: Train batch 5720: loss: 1.24 grad norm: 18.97 time: 1.297
2025-12-05 04:43:18,641: Train batch 5740: loss: 1.40 grad norm: 18.75 time: 1.439
2025-12-05 04:43:46,293: Train batch 5760: loss: 1.28 grad norm: 17.54 time: 1.490
2025-12-05 04:44:15,651: Train batch 5780: loss: 1.48 grad norm: 22.46 time: 1.503
2025-12-05 04:44:45,532: Train batch 5800: loss: 1.15 grad norm: 19.23 time: 1.305
2025-12-05 04:45:13,847: Train batch 5820: loss: 1.40 grad norm: 20.46 time: 1.329
2025-12-05 04:45:42,031: Train batch 5840: loss: 1.46 grad norm: 17.28 time: 1.381
2025-12-05 04:46:09,788: Train batch 5860: loss: 2.13 grad norm: 21.35 time: 1.344
2025-12-05 04:46:39,415: Train batch 5880: loss: 1.15 grad norm: 17.62 time: 1.579
2025-12-05 04:47:07,966: Train batch 5900: loss: 1.92 grad norm: 23.12 time: 1.208
2025-12-05 04:47:36,139: Train batch 5920: loss: 1.24 grad norm: 16.06 time: 1.222
2025-12-05 04:48:04,545: Train batch 5940: loss: 1.88 grad norm: 21.99 time: 1.579
2025-12-05 04:48:34,270: Train batch 5960: loss: 1.46 grad norm: 24.42 time: 1.253
2025-12-05 04:49:01,895: Train batch 5980: loss: 2.16 grad norm: 27.74 time: 1.594
2025-12-05 04:49:30,834: Train batch 6000: loss: 1.74 grad norm: 23.06 time: 1.540
2025-12-05 04:49:30,836: Running test after training batch: 6000
2025-12-05 04:49:37,756: Val batch 6000: PER (avg): 0.0995 CTC Loss (avg): 12.5971 time: 6.917
2025-12-05 04:49:37,756: t15.2023.08.13 val PER: 0.1279
2025-12-05 04:49:37,757: t15.2023.08.18 val PER: 0.0956
2025-12-05 04:49:37,757: t15.2023.08.20 val PER: 0.0969
2025-12-05 04:49:37,757: t15.2023.08.25 val PER: 0.0708
2025-12-05 04:49:37,757: t15.2023.08.27 val PER: 0.1334
2025-12-05 04:49:37,757: t15.2023.09.01 val PER: 0.0657
2025-12-05 04:49:37,758: t15.2023.09.03 val PER: 0.1520
2025-12-05 04:49:37,758: t15.2023.09.24 val PER: 0.1117
2025-12-05 04:49:37,758: t15.2023.09.29 val PER: 0.0916
2025-12-05 04:49:37,758: t15.2023.10.01 val PER: 0.0961
2025-12-05 04:49:37,758: t15.2023.10.06 val PER: 0.0786
2025-12-05 04:49:37,758: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 04:49:37,759: t15.2023.10.13 val PER: 0.0968
2025-12-05 04:49:37,759: t15.2023.10.15 val PER: 0.0908
2025-12-05 04:50:06,591: Train batch 6020: loss: 1.79 grad norm: 20.48 time: 1.372
2025-12-05 04:50:35,444: Train batch 6040: loss: 1.07 grad norm: 16.74 time: 1.374
2025-12-05 04:51:04,952: Train batch 6060: loss: 1.48 grad norm: 20.33 time: 1.382
2025-12-05 04:51:34,111: Train batch 6080: loss: 1.71 grad norm: 19.96 time: 1.417
2025-12-05 04:52:03,117: Train batch 6100: loss: 1.22 grad norm: 16.49 time: 1.509
2025-12-05 04:52:33,294: Train batch 6120: loss: 1.41 grad norm: 18.76 time: 1.823
2025-12-05 04:53:02,447: Train batch 6140: loss: 1.31 grad norm: 18.18 time: 1.477
2025-12-05 04:53:32,417: Train batch 6160: loss: 1.15 grad norm: 18.72 time: 1.536
2025-12-05 04:54:01,839: Train batch 6180: loss: 1.06 grad norm: 18.39 time: 1.159
2025-12-05 04:54:30,533: Train batch 6200: loss: 0.89 grad norm: 15.90 time: 1.358
2025-12-05 04:54:58,935: Train batch 6220: loss: 1.18 grad norm: 19.70 time: 1.476
2025-12-05 04:55:27,262: Train batch 6240: loss: 1.40 grad norm: 20.08 time: 1.254
2025-12-05 04:55:56,114: Train batch 6260: loss: 1.11 grad norm: 16.93 time: 1.219
2025-12-05 04:56:25,541: Train batch 6280: loss: 0.98 grad norm: 15.20 time: 1.395
2025-12-05 04:56:54,887: Train batch 6300: loss: 1.48 grad norm: 15.83 time: 1.398
2025-12-05 04:57:24,956: Train batch 6320: loss: 1.19 grad norm: 17.63 time: 1.557
2025-12-05 04:57:53,047: Train batch 6340: loss: 2.58 grad norm: 20.53 time: 1.370
2025-12-05 04:58:20,888: Train batch 6360: loss: 0.91 grad norm: 17.38 time: 1.359
2025-12-05 04:58:48,774: Train batch 6380: loss: 1.03 grad norm: 14.60 time: 1.537
2025-12-05 04:59:19,524: Train batch 6400: loss: 1.33 grad norm: 20.19 time: 1.638
2025-12-05 04:59:49,210: Train batch 6420: loss: 1.53 grad norm: 24.22 time: 1.730
2025-12-05 05:00:17,663: Train batch 6440: loss: 1.00 grad norm: 18.05 time: 1.444
2025-12-05 05:00:46,725: Train batch 6460: loss: 1.61 grad norm: 21.22 time: 1.298
2025-12-05 05:01:16,351: Train batch 6480: loss: 1.43 grad norm: 14.97 time: 1.444
2025-12-05 05:01:46,077: Train batch 6500: loss: 0.77 grad norm: 12.53 time: 1.168
2025-12-05 05:01:46,079: Running test after training batch: 6500
2025-12-05 05:01:53,665: Val batch 6500: PER (avg): 0.0976 CTC Loss (avg): 12.6333 time: 7.586
2025-12-05 05:01:53,666: t15.2023.08.13 val PER: 0.1289
2025-12-05 05:01:53,666: t15.2023.08.18 val PER: 0.0939
2025-12-05 05:01:53,666: t15.2023.08.20 val PER: 0.0937
2025-12-05 05:01:53,666: t15.2023.08.25 val PER: 0.0753
2025-12-05 05:01:53,667: t15.2023.08.27 val PER: 0.1367
2025-12-05 05:01:53,667: t15.2023.09.01 val PER: 0.0657
2025-12-05 05:01:53,667: t15.2023.09.03 val PER: 0.1401
2025-12-05 05:01:53,667: t15.2023.09.24 val PER: 0.1080
2025-12-05 05:01:53,667: t15.2023.09.29 val PER: 0.0884
2025-12-05 05:01:53,667: t15.2023.10.01 val PER: 0.0928
2025-12-05 05:01:53,668: t15.2023.10.06 val PER: 0.0818
2025-12-05 05:01:53,668: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 05:01:53,668: t15.2023.10.13 val PER: 0.0935
2025-12-05 05:01:53,668: t15.2023.10.15 val PER: 0.0835
2025-12-05 05:01:53,668: New best test PER 0.0981 --> 0.0976
2025-12-05 05:01:53,668: Checkpointing model
2025-12-05 05:01:57,330: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 05:02:26,183: Train batch 6520: loss: 1.29 grad norm: 18.42 time: 1.276
2025-12-05 05:02:55,652: Train batch 6540: loss: 1.31 grad norm: 23.31 time: 1.896
2025-12-05 05:03:23,948: Train batch 6560: loss: 1.16 grad norm: 17.86 time: 1.494
2025-12-05 05:03:53,600: Train batch 6580: loss: 1.54 grad norm: 24.96 time: 1.362
2025-12-05 05:04:23,621: Train batch 6600: loss: 1.72 grad norm: 22.59 time: 1.405
2025-12-05 05:04:51,209: Train batch 6620: loss: 1.23 grad norm: 16.80 time: 1.311
2025-12-05 05:05:19,532: Train batch 6640: loss: 3.33 grad norm: 20.90 time: 1.315
2025-12-05 05:05:50,198: Train batch 6660: loss: 1.02 grad norm: 16.90 time: 1.427
2025-12-05 05:06:18,586: Train batch 6680: loss: 1.29 grad norm: 18.72 time: 1.630
2025-12-05 05:06:47,710: Train batch 6700: loss: 1.99 grad norm: 21.36 time: 1.676
2025-12-05 05:07:15,282: Train batch 6720: loss: 0.68 grad norm: 14.39 time: 1.375
2025-12-05 05:07:45,286: Train batch 6740: loss: 3.25 grad norm: 25.85 time: 1.499
2025-12-05 05:08:15,394: Train batch 6760: loss: 1.78 grad norm: 23.71 time: 1.433
2025-12-05 05:08:43,320: Train batch 6780: loss: 1.15 grad norm: 15.27 time: 1.382
2025-12-05 05:09:11,294: Train batch 6800: loss: 1.34 grad norm: 18.33 time: 1.573
2025-12-05 05:09:41,036: Train batch 6820: loss: 1.21 grad norm: 18.47 time: 1.591
2025-12-05 05:10:10,199: Train batch 6840: loss: 0.84 grad norm: 13.46 time: 1.294
2025-12-05 05:10:39,609: Train batch 6860: loss: 1.37 grad norm: 23.01 time: 1.282
2025-12-05 05:11:08,310: Train batch 6880: loss: 1.22 grad norm: 17.54 time: 1.509
2025-12-05 05:11:37,774: Train batch 6900: loss: 1.00 grad norm: 18.85 time: 1.878
2025-12-05 05:12:06,613: Train batch 6920: loss: 1.52 grad norm: 21.68 time: 1.398
2025-12-05 05:12:36,269: Train batch 6940: loss: 1.27 grad norm: 18.60 time: 1.531
2025-12-05 05:13:05,471: Train batch 6960: loss: 1.46 grad norm: 21.75 time: 1.382
2025-12-05 05:13:35,938: Train batch 6980: loss: 1.35 grad norm: 20.24 time: 1.405
2025-12-05 05:14:04,386: Train batch 7000: loss: 1.39 grad norm: 19.94 time: 1.380
2025-12-05 05:14:04,387: Running test after training batch: 7000
2025-12-05 05:14:12,325: Val batch 7000: PER (avg): 0.0978 CTC Loss (avg): 12.9308 time: 7.935
2025-12-05 05:14:12,325: t15.2023.08.13 val PER: 0.1237
2025-12-05 05:14:12,326: t15.2023.08.18 val PER: 0.0922
2025-12-05 05:14:12,326: t15.2023.08.20 val PER: 0.0953
2025-12-05 05:14:12,326: t15.2023.08.25 val PER: 0.0693
2025-12-05 05:14:12,326: t15.2023.08.27 val PER: 0.1415
2025-12-05 05:14:12,326: t15.2023.09.01 val PER: 0.0641
2025-12-05 05:14:12,327: t15.2023.09.03 val PER: 0.1437
2025-12-05 05:14:12,327: t15.2023.09.24 val PER: 0.1117
2025-12-05 05:14:12,327: t15.2023.09.29 val PER: 0.0868
2025-12-05 05:14:12,327: t15.2023.10.01 val PER: 0.0945
2025-12-05 05:14:12,327: t15.2023.10.06 val PER: 0.0840
2025-12-05 05:14:12,327: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 05:14:12,328: t15.2023.10.13 val PER: 0.0952
2025-12-05 05:14:12,328: t15.2023.10.15 val PER: 0.0849
2025-12-05 05:14:43,790: Train batch 7020: loss: 1.73 grad norm: 22.64 time: 1.425
2025-12-05 05:15:12,323: Train batch 7040: loss: 1.78 grad norm: 20.79 time: 1.688
2025-12-05 05:15:42,367: Train batch 7060: loss: 1.10 grad norm: 19.79 time: 1.556
2025-12-05 05:16:10,932: Train batch 7080: loss: 1.01 grad norm: 15.91 time: 2.357
2025-12-05 05:16:39,489: Train batch 7100: loss: 1.37 grad norm: 17.95 time: 1.351
2025-12-05 05:17:08,242: Train batch 7120: loss: 1.43 grad norm: 20.88 time: 1.581
2025-12-05 05:17:38,901: Train batch 7140: loss: 1.61 grad norm: 22.01 time: 1.518
2025-12-05 05:18:06,061: Train batch 7160: loss: 2.34 grad norm: 30.09 time: 1.329
2025-12-05 05:18:33,993: Train batch 7180: loss: 1.23 grad norm: 20.35 time: 1.328
2025-12-05 05:19:03,850: Train batch 7200: loss: 0.87 grad norm: 15.73 time: 1.596
2025-12-05 05:19:31,630: Train batch 7220: loss: 1.44 grad norm: 17.53 time: 1.582
2025-12-05 05:19:59,343: Train batch 7240: loss: 1.57 grad norm: 21.57 time: 1.337
2025-12-05 05:20:27,896: Train batch 7260: loss: 0.79 grad norm: 14.03 time: 1.474
2025-12-05 05:20:56,314: Train batch 7280: loss: 1.02 grad norm: 16.17 time: 1.428
2025-12-05 05:21:25,526: Train batch 7300: loss: 1.84 grad norm: 34.23 time: 1.324
2025-12-05 05:21:53,669: Train batch 7320: loss: 1.14 grad norm: 20.19 time: 1.488
2025-12-05 05:22:22,337: Train batch 7340: loss: 0.73 grad norm: 15.47 time: 1.369
2025-12-05 05:22:53,028: Train batch 7360: loss: 1.18 grad norm: 26.94 time: 1.584
2025-12-05 05:23:22,394: Train batch 7380: loss: 1.02 grad norm: 16.55 time: 1.483
2025-12-05 05:23:51,191: Train batch 7400: loss: 1.22 grad norm: 18.47 time: 1.379
2025-12-05 05:24:20,670: Train batch 7420: loss: 1.35 grad norm: 21.94 time: 1.831
2025-12-05 05:24:49,929: Train batch 7440: loss: 1.29 grad norm: 20.37 time: 1.427
2025-12-05 05:25:17,403: Train batch 7460: loss: 0.96 grad norm: 18.05 time: 1.379
2025-12-05 05:25:45,728: Train batch 7480: loss: 1.70 grad norm: 22.09 time: 1.370
2025-12-05 05:26:14,408: Train batch 7500: loss: 1.85 grad norm: 22.51 time: 1.525
2025-12-05 05:26:14,411: Running test after training batch: 7500
2025-12-05 05:26:21,333: Val batch 7500: PER (avg): 0.0977 CTC Loss (avg): 12.7220 time: 6.920
2025-12-05 05:26:21,333: t15.2023.08.13 val PER: 0.1237
2025-12-05 05:26:21,334: t15.2023.08.18 val PER: 0.0922
2025-12-05 05:26:21,334: t15.2023.08.20 val PER: 0.0953
2025-12-05 05:26:21,334: t15.2023.08.25 val PER: 0.0663
2025-12-05 05:26:21,334: t15.2023.08.27 val PER: 0.1334
2025-12-05 05:26:21,334: t15.2023.09.01 val PER: 0.0609
2025-12-05 05:26:21,334: t15.2023.09.03 val PER: 0.1568
2025-12-05 05:26:21,335: t15.2023.09.24 val PER: 0.1117
2025-12-05 05:26:21,335: t15.2023.09.29 val PER: 0.0916
2025-12-05 05:26:21,335: t15.2023.10.01 val PER: 0.0961
2025-12-05 05:26:21,335: t15.2023.10.06 val PER: 0.0818
2025-12-05 05:26:21,335: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 05:26:21,336: t15.2023.10.13 val PER: 0.0919
2025-12-05 05:26:21,336: t15.2023.10.15 val PER: 0.0835
2025-12-05 05:26:49,399: Train batch 7520: loss: 1.13 grad norm: 19.06 time: 1.198
2025-12-05 05:27:18,746: Train batch 7540: loss: 1.52 grad norm: 22.00 time: 1.361
2025-12-05 05:27:48,203: Train batch 7560: loss: 0.97 grad norm: 17.71 time: 1.505
2025-12-05 05:28:17,333: Train batch 7580: loss: 1.50 grad norm: 20.55 time: 1.382
2025-12-05 05:28:46,945: Train batch 7600: loss: 1.12 grad norm: 15.95 time: 1.535
2025-12-05 05:29:16,417: Train batch 7620: loss: 1.60 grad norm: 22.12 time: 1.674
2025-12-05 05:29:46,964: Train batch 7640: loss: 1.17 grad norm: 18.09 time: 1.632
2025-12-05 05:30:16,154: Train batch 7660: loss: 0.92 grad norm: 17.68 time: 1.390
2025-12-05 05:30:44,757: Train batch 7680: loss: 1.57 grad norm: 20.41 time: 1.639
2025-12-05 05:31:13,865: Train batch 7700: loss: 1.47 grad norm: 22.51 time: 1.407
2025-12-05 05:31:41,190: Train batch 7720: loss: 0.67 grad norm: 12.76 time: 1.567
2025-12-05 05:32:08,593: Train batch 7740: loss: 1.25 grad norm: 21.74 time: 1.355
2025-12-05 05:32:36,779: Train batch 7760: loss: 1.35 grad norm: 20.85 time: 1.573
2025-12-05 05:33:06,974: Train batch 7780: loss: 0.92 grad norm: 15.80 time: 1.299
2025-12-05 05:33:34,619: Train batch 7800: loss: 1.35 grad norm: 22.03 time: 1.416
2025-12-05 05:34:03,932: Train batch 7820: loss: 1.21 grad norm: 20.42 time: 1.639
2025-12-05 05:34:33,069: Train batch 7840: loss: 1.05 grad norm: 17.94 time: 1.835
2025-12-05 05:35:01,601: Train batch 7860: loss: 1.21 grad norm: 21.31 time: 1.261
2025-12-05 05:35:29,617: Train batch 7880: loss: 0.98 grad norm: 18.50 time: 1.407
2025-12-05 05:35:57,069: Train batch 7900: loss: 1.52 grad norm: 22.96 time: 1.238
2025-12-05 05:36:26,676: Train batch 7920: loss: 1.34 grad norm: 18.80 time: 1.277
2025-12-05 05:36:56,311: Train batch 7940: loss: 1.21 grad norm: 17.92 time: 1.304
2025-12-05 05:37:24,729: Train batch 7960: loss: 1.07 grad norm: 17.86 time: 1.442
2025-12-05 05:37:55,279: Train batch 7980: loss: 1.99 grad norm: 21.39 time: 1.223
2025-12-05 05:38:24,084: Train batch 8000: loss: 0.91 grad norm: 15.80 time: 1.440
2025-12-05 05:38:24,088: Running test after training batch: 8000
2025-12-05 05:38:31,761: Val batch 8000: PER (avg): 0.0958 CTC Loss (avg): 13.0183 time: 7.669
2025-12-05 05:38:31,761: t15.2023.08.13 val PER: 0.1206
2025-12-05 05:38:31,762: t15.2023.08.18 val PER: 0.0922
2025-12-05 05:38:31,762: t15.2023.08.20 val PER: 0.0921
2025-12-05 05:38:31,762: t15.2023.08.25 val PER: 0.0633
2025-12-05 05:38:31,762: t15.2023.08.27 val PER: 0.1383
2025-12-05 05:38:31,763: t15.2023.09.01 val PER: 0.0584
2025-12-05 05:38:31,763: t15.2023.09.03 val PER: 0.1425
2025-12-05 05:38:31,763: t15.2023.09.24 val PER: 0.1153
2025-12-05 05:38:31,763: t15.2023.09.29 val PER: 0.0916
2025-12-05 05:38:31,763: t15.2023.10.01 val PER: 0.0912
2025-12-05 05:38:31,763: t15.2023.10.06 val PER: 0.0807
2025-12-05 05:38:31,764: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 05:38:31,764: t15.2023.10.13 val PER: 0.0952
2025-12-05 05:38:31,764: t15.2023.10.15 val PER: 0.0820
2025-12-05 05:38:31,764: New best test PER 0.0976 --> 0.0958
2025-12-05 05:38:31,764: Checkpointing model
2025-12-05 05:38:36,067: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 05:39:04,596: Train batch 8020: loss: 1.49 grad norm: 21.75 time: 1.523
2025-12-05 05:39:33,136: Train batch 8040: loss: 0.62 grad norm: 11.25 time: 1.288
2025-12-05 05:40:01,826: Train batch 8060: loss: 1.05 grad norm: 15.30 time: 1.425
2025-12-05 05:40:32,088: Train batch 8080: loss: 1.55 grad norm: 22.57 time: 1.314
2025-12-05 05:41:00,858: Train batch 8100: loss: 1.33 grad norm: 17.27 time: 1.400
2025-12-05 05:41:30,994: Train batch 8120: loss: 1.64 grad norm: 20.96 time: 1.551
2025-12-05 05:42:00,697: Train batch 8140: loss: 1.51 grad norm: 25.62 time: 1.607
2025-12-05 05:42:29,856: Train batch 8160: loss: 1.59 grad norm: 19.49 time: 1.357
2025-12-05 05:42:58,338: Train batch 8180: loss: 0.98 grad norm: 14.83 time: 1.379
2025-12-05 05:43:27,271: Train batch 8200: loss: 1.10 grad norm: 17.57 time: 1.377
2025-12-05 05:43:57,241: Train batch 8220: loss: 0.87 grad norm: 14.06 time: 1.564
2025-12-05 05:44:25,234: Train batch 8240: loss: 1.25 grad norm: 19.97 time: 1.902
2025-12-05 05:44:52,923: Train batch 8260: loss: 1.72 grad norm: 25.04 time: 1.297
2025-12-05 05:45:23,921: Train batch 8280: loss: 1.01 grad norm: 20.29 time: 1.410
2025-12-05 05:45:54,264: Train batch 8300: loss: 1.38 grad norm: 21.06 time: 1.446
2025-12-05 05:46:23,124: Train batch 8320: loss: 1.18 grad norm: 22.01 time: 1.542
2025-12-05 05:46:51,807: Train batch 8340: loss: 0.76 grad norm: 12.87 time: 1.335
2025-12-05 05:47:21,356: Train batch 8360: loss: 0.88 grad norm: 16.28 time: 1.493
2025-12-05 05:47:49,976: Train batch 8380: loss: 1.19 grad norm: 21.73 time: 1.525
2025-12-05 05:48:19,048: Train batch 8400: loss: 0.69 grad norm: 12.39 time: 1.231
2025-12-05 05:48:47,733: Train batch 8420: loss: 1.08 grad norm: 17.44 time: 1.315
2025-12-05 05:49:16,451: Train batch 8440: loss: 0.82 grad norm: 15.38 time: 1.377
2025-12-05 05:49:45,204: Train batch 8460: loss: 1.33 grad norm: 23.13 time: 1.352
2025-12-05 05:50:15,033: Train batch 8480: loss: 1.41 grad norm: 13.92 time: 1.379
2025-12-05 05:50:44,542: Train batch 8500: loss: 0.77 grad norm: 17.69 time: 1.338
2025-12-05 05:50:44,545: Running test after training batch: 8500
2025-12-05 05:50:51,494: Val batch 8500: PER (avg): 0.0966 CTC Loss (avg): 13.0593 time: 6.948
2025-12-05 05:50:51,495: t15.2023.08.13 val PER: 0.1268
2025-12-05 05:50:51,495: t15.2023.08.18 val PER: 0.0897
2025-12-05 05:50:51,495: t15.2023.08.20 val PER: 0.0898
2025-12-05 05:50:51,495: t15.2023.08.25 val PER: 0.0617
2025-12-05 05:50:51,496: t15.2023.08.27 val PER: 0.1415
2025-12-05 05:50:51,496: t15.2023.09.01 val PER: 0.0609
2025-12-05 05:50:51,496: t15.2023.09.03 val PER: 0.1473
2025-12-05 05:50:51,496: t15.2023.09.24 val PER: 0.1019
2025-12-05 05:50:51,496: t15.2023.09.29 val PER: 0.1013
2025-12-05 05:50:51,496: t15.2023.10.01 val PER: 0.0928
2025-12-05 05:50:51,496: t15.2023.10.06 val PER: 0.0840
2025-12-05 05:50:51,497: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 05:50:51,497: t15.2023.10.13 val PER: 0.0871
2025-12-05 05:50:51,497: t15.2023.10.15 val PER: 0.0922
2025-12-05 05:51:19,423: Train batch 8520: loss: 1.42 grad norm: 22.30 time: 1.359
2025-12-05 05:51:49,322: Train batch 8540: loss: 1.19 grad norm: 23.55 time: 1.393
2025-12-05 05:52:19,307: Train batch 8560: loss: 1.29 grad norm: 23.89 time: 1.192
2025-12-05 05:52:48,249: Train batch 8580: loss: 1.04 grad norm: 16.90 time: 1.496
2025-12-05 05:53:17,273: Train batch 8600: loss: 0.87 grad norm: 16.08 time: 1.710
2025-12-05 05:53:45,436: Train batch 8620: loss: 1.81 grad norm: 19.64 time: 1.373
2025-12-05 05:54:14,299: Train batch 8640: loss: 2.72 grad norm: 24.35 time: 1.622
2025-12-05 05:54:42,065: Train batch 8660: loss: 0.77 grad norm: 14.09 time: 1.635
2025-12-05 05:55:11,701: Train batch 8680: loss: 1.15 grad norm: 18.29 time: 1.399
2025-12-05 05:55:40,700: Train batch 8700: loss: 1.35 grad norm: 20.69 time: 1.620
2025-12-05 05:56:09,533: Train batch 8720: loss: 0.90 grad norm: 15.58 time: 1.545
2025-12-05 05:56:39,792: Train batch 8740: loss: 1.18 grad norm: 19.04 time: 1.685
2025-12-05 05:57:08,234: Train batch 8760: loss: 1.11 grad norm: 17.65 time: 1.205
2025-12-05 05:57:37,587: Train batch 8780: loss: 1.17 grad norm: 18.16 time: 1.567
2025-12-05 05:58:06,929: Train batch 8800: loss: 1.37 grad norm: 20.80 time: 1.325
2025-12-05 05:58:36,813: Train batch 8820: loss: 0.65 grad norm: 17.78 time: 1.331
2025-12-05 05:59:04,853: Train batch 8840: loss: 0.73 grad norm: 17.79 time: 1.315
2025-12-05 05:59:32,674: Train batch 8860: loss: 1.19 grad norm: 18.20 time: 1.334
2025-12-05 06:00:02,321: Train batch 8880: loss: 0.79 grad norm: 14.12 time: 1.293
2025-12-05 06:00:30,971: Train batch 8900: loss: 1.20 grad norm: 14.70 time: 1.514
2025-12-05 06:00:59,823: Train batch 8920: loss: 0.88 grad norm: 17.78 time: 1.424
2025-12-05 06:01:29,367: Train batch 8940: loss: 0.67 grad norm: 12.78 time: 1.313
2025-12-05 06:01:58,040: Train batch 8960: loss: 0.72 grad norm: 16.47 time: 1.502
2025-12-05 06:02:26,179: Train batch 8980: loss: 0.91 grad norm: 17.62 time: 1.571
2025-12-05 06:02:54,987: Train batch 9000: loss: 1.15 grad norm: 22.02 time: 1.723
2025-12-05 06:02:54,989: Running test after training batch: 9000
2025-12-05 06:03:01,997: Val batch 9000: PER (avg): 0.0962 CTC Loss (avg): 12.9851 time: 7.007
2025-12-05 06:03:01,998: t15.2023.08.13 val PER: 0.1310
2025-12-05 06:03:01,998: t15.2023.08.18 val PER: 0.0897
2025-12-05 06:03:01,998: t15.2023.08.20 val PER: 0.0921
2025-12-05 06:03:01,998: t15.2023.08.25 val PER: 0.0723
2025-12-05 06:03:01,998: t15.2023.08.27 val PER: 0.1383
2025-12-05 06:03:01,998: t15.2023.09.01 val PER: 0.0601
2025-12-05 06:03:01,998: t15.2023.09.03 val PER: 0.1425
2025-12-05 06:03:01,999: t15.2023.09.24 val PER: 0.1032
2025-12-05 06:03:01,999: t15.2023.09.29 val PER: 0.0965
2025-12-05 06:03:01,999: t15.2023.10.01 val PER: 0.0961
2025-12-05 06:03:01,999: t15.2023.10.06 val PER: 0.0807
2025-12-05 06:03:01,999: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 06:03:01,999: t15.2023.10.13 val PER: 0.0871
2025-12-05 06:03:01,999: t15.2023.10.15 val PER: 0.0791
2025-12-05 06:03:31,273: Train batch 9020: loss: 1.24 grad norm: 17.42 time: 1.490
2025-12-05 06:04:00,812: Train batch 9040: loss: 2.63 grad norm: 20.02 time: 1.634
2025-12-05 06:04:29,059: Train batch 9060: loss: 0.65 grad norm: 13.98 time: 1.564
2025-12-05 06:04:57,993: Train batch 9080: loss: 1.04 grad norm: 17.09 time: 1.259
2025-12-05 06:05:26,968: Train batch 9100: loss: 0.80 grad norm: 14.28 time: 1.253
2025-12-05 06:05:56,341: Train batch 9120: loss: 1.81 grad norm: 28.46 time: 1.268
2025-12-05 06:06:26,082: Train batch 9140: loss: 0.72 grad norm: 14.55 time: 1.735
2025-12-05 06:07:00,042: Train batch 9160: loss: 1.49 grad norm: 24.12 time: 1.678
2025-12-05 06:07:39,599: Train batch 9180: loss: 0.78 grad norm: 13.44 time: 2.105
2025-12-05 06:08:20,390: Train batch 9200: loss: 0.97 grad norm: 17.31 time: 1.616
2025-12-05 06:09:01,251: Train batch 9220: loss: 0.87 grad norm: 18.30 time: 1.628
2025-12-05 06:09:41,021: Train batch 9240: loss: 0.82 grad norm: 15.25 time: 2.108
2025-12-05 06:10:19,857: Train batch 9260: loss: 1.29 grad norm: 26.68 time: 1.668
2025-12-05 06:10:51,670: Train batch 9280: loss: 0.92 grad norm: 15.60 time: 1.768
2025-12-05 06:11:21,888: Train batch 9300: loss: 0.69 grad norm: 12.05 time: 1.385
2025-12-05 06:11:49,635: Train batch 9320: loss: 1.28 grad norm: 20.97 time: 1.392
2025-12-05 06:12:18,300: Train batch 9340: loss: 0.97 grad norm: 16.39 time: 1.299
2025-12-05 06:12:46,191: Train batch 9360: loss: 1.07 grad norm: 18.26 time: 1.338
2025-12-05 06:13:14,810: Train batch 9380: loss: 1.02 grad norm: 17.75 time: 1.600
2025-12-05 06:13:42,390: Train batch 9400: loss: 1.02 grad norm: 18.72 time: 1.503
2025-12-05 06:14:11,231: Train batch 9420: loss: 1.04 grad norm: 21.63 time: 1.532
2025-12-05 06:14:39,727: Train batch 9440: loss: 1.23 grad norm: 20.66 time: 1.579
2025-12-05 06:15:06,935: Train batch 9460: loss: 0.98 grad norm: 16.56 time: 1.327
2025-12-05 06:15:35,359: Train batch 9480: loss: 1.28 grad norm: 21.89 time: 1.608
2025-12-05 06:16:04,179: Train batch 9500: loss: 1.22 grad norm: 18.78 time: 1.592
2025-12-05 06:16:04,182: Running test after training batch: 9500
2025-12-05 06:16:11,110: Val batch 9500: PER (avg): 0.0968 CTC Loss (avg): 13.4350 time: 6.926
2025-12-05 06:16:11,110: t15.2023.08.13 val PER: 0.1247
2025-12-05 06:16:11,110: t15.2023.08.18 val PER: 0.0897
2025-12-05 06:16:11,111: t15.2023.08.20 val PER: 0.0882
2025-12-05 06:16:11,111: t15.2023.08.25 val PER: 0.0738
2025-12-05 06:16:11,111: t15.2023.08.27 val PER: 0.1463
2025-12-05 06:16:11,111: t15.2023.09.01 val PER: 0.0641
2025-12-05 06:16:11,111: t15.2023.09.03 val PER: 0.1449
2025-12-05 06:16:11,111: t15.2023.09.24 val PER: 0.1007
2025-12-05 06:16:11,112: t15.2023.09.29 val PER: 0.0884
2025-12-05 06:16:11,112: t15.2023.10.01 val PER: 0.0993
2025-12-05 06:16:11,112: t15.2023.10.06 val PER: 0.0861
2025-12-05 06:16:11,112: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 06:16:11,112: t15.2023.10.13 val PER: 0.0952
2025-12-05 06:16:11,112: t15.2023.10.15 val PER: 0.0791
2025-12-05 06:16:38,891: Train batch 9520: loss: 0.73 grad norm: 14.40 time: 1.424
2025-12-05 06:17:07,276: Train batch 9540: loss: 0.69 grad norm: 18.08 time: 1.370
2025-12-05 06:17:37,581: Train batch 9560: loss: 0.69 grad norm: 17.23 time: 1.380
2025-12-05 06:18:06,153: Train batch 9580: loss: 1.10 grad norm: 20.64 time: 1.434
2025-12-05 06:18:35,207: Train batch 9600: loss: 2.34 grad norm: 22.17 time: 1.830
2025-12-05 06:19:04,890: Train batch 9620: loss: 0.87 grad norm: 16.49 time: 1.369
2025-12-05 06:19:35,769: Train batch 9640: loss: 0.67 grad norm: 14.29 time: 1.538
2025-12-05 06:20:03,984: Train batch 9660: loss: 1.03 grad norm: 17.82 time: 1.447
2025-12-05 06:20:32,931: Train batch 9680: loss: 1.11 grad norm: 23.39 time: 1.548
2025-12-05 06:21:01,141: Train batch 9700: loss: 1.35 grad norm: 20.66 time: 1.322
2025-12-05 06:21:30,452: Train batch 9720: loss: 2.23 grad norm: 15.28 time: 1.275
2025-12-05 06:22:00,167: Train batch 9740: loss: 0.63 grad norm: 13.47 time: 1.272
2025-12-05 06:22:28,118: Train batch 9760: loss: 1.06 grad norm: 18.74 time: 1.557
2025-12-05 06:22:58,068: Train batch 9780: loss: 0.80 grad norm: 13.49 time: 1.584
2025-12-05 06:23:27,508: Train batch 9800: loss: 1.33 grad norm: 22.66 time: 1.362
2025-12-05 06:23:55,755: Train batch 9820: loss: 0.92 grad norm: 16.83 time: 1.381
2025-12-05 06:24:24,737: Train batch 9840: loss: 1.52 grad norm: 16.66 time: 1.578
2025-12-05 06:25:04,672: Train batch 9860: loss: 0.97 grad norm: 20.31 time: 1.227
2025-12-05 06:27:08,744: Train batch 9880: loss: 0.95 grad norm: 16.72 time: 1.429
2025-12-05 06:28:38,432: Train batch 9900: loss: 1.54 grad norm: 16.32 time: 1.225
2025-12-05 06:29:46,923: Train batch 9920: loss: 1.46 grad norm: 21.24 time: 1.559
2025-12-05 06:30:32,801: Train batch 9940: loss: 1.18 grad norm: 18.85 time: 1.285
2025-12-05 06:31:09,857: Train batch 9960: loss: 1.17 grad norm: 19.28 time: 1.626
2025-12-05 06:31:41,768: Train batch 9980: loss: 1.83 grad norm: 21.31 time: 1.430
2025-12-05 06:32:12,162: Train batch 10000: loss: 0.65 grad norm: 12.48 time: 1.307
2025-12-05 06:32:12,163: Running test after training batch: 10000
2025-12-05 06:32:29,703: Val batch 10000: PER (avg): 0.0962 CTC Loss (avg): 13.5888 time: 17.537
2025-12-05 06:32:29,703: t15.2023.08.13 val PER: 0.1195
2025-12-05 06:32:29,703: t15.2023.08.18 val PER: 0.0863
2025-12-05 06:32:29,703: t15.2023.08.20 val PER: 0.0874
2025-12-05 06:32:29,703: t15.2023.08.25 val PER: 0.0768
2025-12-05 06:32:29,704: t15.2023.08.27 val PER: 0.1463
2025-12-05 06:32:29,704: t15.2023.09.01 val PER: 0.0657
2025-12-05 06:32:29,704: t15.2023.09.03 val PER: 0.1390
2025-12-05 06:32:29,704: t15.2023.09.24 val PER: 0.1056
2025-12-05 06:32:29,704: t15.2023.09.29 val PER: 0.0965
2025-12-05 06:32:29,704: t15.2023.10.01 val PER: 0.0928
2025-12-05 06:32:29,704: t15.2023.10.06 val PER: 0.0840
2025-12-05 06:32:29,705: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 06:32:29,705: t15.2023.10.13 val PER: 0.0952
2025-12-05 06:32:29,705: t15.2023.10.15 val PER: 0.0820
2025-12-05 06:33:30,996: Train batch 10020: loss: 0.92 grad norm: 18.70 time: 2.018
2025-12-05 06:34:39,138: Train batch 10040: loss: 1.15 grad norm: 23.09 time: 2.552
2025-12-05 06:35:33,202: Train batch 10060: loss: 0.65 grad norm: 17.72 time: 3.079
2025-12-05 06:36:22,565: Train batch 10080: loss: 1.00 grad norm: 16.80 time: 2.860
2025-12-05 06:37:15,596: Train batch 10100: loss: 1.15 grad norm: 18.10 time: 2.436
2025-12-05 06:38:04,263: Train batch 10120: loss: 1.13 grad norm: 19.94 time: 2.742
2025-12-05 06:38:53,375: Train batch 10140: loss: 0.87 grad norm: 19.70 time: 2.507
2025-12-05 06:39:44,878: Train batch 10160: loss: 0.74 grad norm: 16.60 time: 2.545
2025-12-05 06:40:37,429: Train batch 10180: loss: 1.06 grad norm: 20.38 time: 2.751
2025-12-05 06:41:27,370: Train batch 10200: loss: 0.97 grad norm: 18.47 time: 2.740
2025-12-05 06:42:18,504: Train batch 10220: loss: 0.74 grad norm: 15.33 time: 2.335
2025-12-05 06:43:07,927: Train batch 10240: loss: 0.93 grad norm: 19.61 time: 3.117
2025-12-05 06:43:59,334: Train batch 10260: loss: 0.96 grad norm: 18.39 time: 2.563
2025-12-05 06:44:50,998: Train batch 10280: loss: 1.12 grad norm: 20.65 time: 2.363
2025-12-05 06:45:41,798: Train batch 10300: loss: 0.49 grad norm: 10.21 time: 3.102
2025-12-05 06:46:33,219: Train batch 10320: loss: 1.27 grad norm: 26.09 time: 2.735
2025-12-05 06:47:23,831: Train batch 10340: loss: 0.83 grad norm: 14.44 time: 2.605
2025-12-05 06:48:13,937: Train batch 10360: loss: 0.65 grad norm: 15.13 time: 1.894
2025-12-05 06:48:57,860: Train batch 10380: loss: 0.60 grad norm: 15.08 time: 1.312
2025-12-05 06:49:28,436: Train batch 10400: loss: 0.83 grad norm: 16.06 time: 1.633
2025-12-05 06:49:56,355: Train batch 10420: loss: 0.44 grad norm: 12.22 time: 1.334
2025-12-05 06:50:25,546: Train batch 10440: loss: 0.78 grad norm: 16.88 time: 1.927
2025-12-05 06:50:54,589: Train batch 10460: loss: 1.22 grad norm: 13.60 time: 1.274
2025-12-05 06:51:23,421: Train batch 10480: loss: 0.60 grad norm: 14.68 time: 1.824
2025-12-05 06:51:52,651: Train batch 10500: loss: 0.78 grad norm: 13.40 time: 1.415
2025-12-05 06:51:52,653: Running test after training batch: 10500
2025-12-05 06:52:10,048: Val batch 10500: PER (avg): 0.0967 CTC Loss (avg): 13.5704 time: 17.393
2025-12-05 06:52:10,049: t15.2023.08.13 val PER: 0.1237
2025-12-05 06:52:10,049: t15.2023.08.18 val PER: 0.0939
2025-12-05 06:52:10,049: t15.2023.08.20 val PER: 0.0882
2025-12-05 06:52:10,049: t15.2023.08.25 val PER: 0.0783
2025-12-05 06:52:10,049: t15.2023.08.27 val PER: 0.1367
2025-12-05 06:52:10,049: t15.2023.09.01 val PER: 0.0649
2025-12-05 06:52:10,050: t15.2023.09.03 val PER: 0.1390
2025-12-05 06:52:10,050: t15.2023.09.24 val PER: 0.1007
2025-12-05 06:52:10,050: t15.2023.09.29 val PER: 0.0965
2025-12-05 06:52:10,050: t15.2023.10.01 val PER: 0.0928
2025-12-05 06:52:10,050: t15.2023.10.06 val PER: 0.0797
2025-12-05 06:52:10,050: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 06:52:10,050: t15.2023.10.13 val PER: 0.1000
2025-12-05 06:52:10,051: t15.2023.10.15 val PER: 0.0849
2025-12-05 06:52:38,777: Train batch 10520: loss: 0.73 grad norm: 18.01 time: 1.709
2025-12-05 06:53:07,386: Train batch 10540: loss: 0.78 grad norm: 16.28 time: 1.309
2025-12-05 06:53:36,816: Train batch 10560: loss: 1.01 grad norm: 19.15 time: 1.913
2025-12-05 06:54:05,869: Train batch 10580: loss: 1.41 grad norm: 15.82 time: 1.353
2025-12-05 06:54:37,892: Train batch 10600: loss: 1.01 grad norm: 18.46 time: 1.403
2025-12-05 06:55:07,937: Train batch 10620: loss: 1.25 grad norm: 21.46 time: 1.563
2025-12-05 06:55:38,264: Train batch 10640: loss: 1.47 grad norm: 15.32 time: 1.721
2025-12-05 06:56:08,412: Train batch 10660: loss: 0.80 grad norm: 12.70 time: 1.312
2025-12-05 06:56:39,987: Train batch 10680: loss: 1.34 grad norm: 21.86 time: 1.715
2025-12-05 06:57:09,986: Train batch 10700: loss: 1.06 grad norm: 20.12 time: 1.487
2025-12-05 06:57:39,057: Train batch 10720: loss: 0.72 grad norm: 12.74 time: 1.352
2025-12-05 06:58:07,212: Train batch 10740: loss: 0.67 grad norm: 14.25 time: 1.358
2025-12-05 06:58:35,867: Train batch 10760: loss: 1.32 grad norm: 22.38 time: 1.555
2025-12-05 06:59:04,614: Train batch 10780: loss: 1.24 grad norm: 19.93 time: 1.384
2025-12-05 06:59:33,169: Train batch 10800: loss: 0.53 grad norm: 14.79 time: 1.327
2025-12-05 07:00:01,109: Train batch 10820: loss: 0.80 grad norm: 20.68 time: 1.520
2025-12-05 07:00:29,766: Train batch 10840: loss: 0.52 grad norm: 10.17 time: 1.459
2025-12-05 07:01:00,675: Train batch 10860: loss: 0.72 grad norm: 16.31 time: 1.572
2025-12-05 07:01:30,048: Train batch 10880: loss: 0.57 grad norm: 10.32 time: 1.270
2025-12-05 07:01:58,582: Train batch 10900: loss: 1.19 grad norm: 19.50 time: 1.331
2025-12-05 07:02:26,340: Train batch 10920: loss: 1.08 grad norm: 20.31 time: 1.196
2025-12-05 07:02:55,355: Train batch 10940: loss: 1.06 grad norm: 16.40 time: 1.147
2025-12-05 07:03:22,931: Train batch 10960: loss: 0.69 grad norm: 15.24 time: 1.542
2025-12-05 07:03:52,229: Train batch 10980: loss: 1.29 grad norm: 17.84 time: 1.586
2025-12-05 07:04:21,537: Train batch 11000: loss: 1.22 grad norm: 19.53 time: 1.486
2025-12-05 07:04:21,540: Running test after training batch: 11000
2025-12-05 07:04:28,528: Val batch 11000: PER (avg): 0.0948 CTC Loss (avg): 13.7940 time: 6.986
2025-12-05 07:04:28,528: t15.2023.08.13 val PER: 0.1268
2025-12-05 07:04:28,528: t15.2023.08.18 val PER: 0.0872
2025-12-05 07:04:28,528: t15.2023.08.20 val PER: 0.0882
2025-12-05 07:04:28,529: t15.2023.08.25 val PER: 0.0738
2025-12-05 07:04:28,529: t15.2023.08.27 val PER: 0.1399
2025-12-05 07:04:28,529: t15.2023.09.01 val PER: 0.0609
2025-12-05 07:04:28,529: t15.2023.09.03 val PER: 0.1425
2025-12-05 07:04:28,529: t15.2023.09.24 val PER: 0.0959
2025-12-05 07:04:28,530: t15.2023.09.29 val PER: 0.0884
2025-12-05 07:04:28,530: t15.2023.10.01 val PER: 0.0945
2025-12-05 07:04:28,530: t15.2023.10.06 val PER: 0.0797
2025-12-05 07:04:28,530: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 07:04:28,530: t15.2023.10.13 val PER: 0.0919
2025-12-05 07:04:28,530: t15.2023.10.15 val PER: 0.0849
2025-12-05 07:04:28,531: New best test PER 0.0958 --> 0.0948
2025-12-05 07:04:28,531: Checkpointing model
2025-12-05 07:04:32,330: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 07:05:00,524: Train batch 11020: loss: 0.92 grad norm: 19.16 time: 1.369
2025-12-05 07:05:28,728: Train batch 11040: loss: 1.18 grad norm: 24.25 time: 1.537
2025-12-05 07:05:58,094: Train batch 11060: loss: 1.23 grad norm: 22.04 time: 1.571
2025-12-05 07:06:26,762: Train batch 11080: loss: 0.83 grad norm: 15.37 time: 1.316
2025-12-05 07:06:54,890: Train batch 11100: loss: 1.05 grad norm: 23.83 time: 1.522
2025-12-05 07:07:24,012: Train batch 11120: loss: 0.88 grad norm: 18.47 time: 1.187
2025-12-05 07:07:53,080: Train batch 11140: loss: 0.87 grad norm: 16.29 time: 1.357
2025-12-05 07:08:21,932: Train batch 11160: loss: 0.65 grad norm: 13.96 time: 1.318
2025-12-05 07:08:49,731: Train batch 11180: loss: 2.30 grad norm: 22.50 time: 1.342
2025-12-05 07:09:18,585: Train batch 11200: loss: 1.37 grad norm: 31.49 time: 1.358
2025-12-05 07:09:47,080: Train batch 11220: loss: 0.59 grad norm: 16.86 time: 1.315
2025-12-05 07:10:16,938: Train batch 11240: loss: 0.69 grad norm: 15.45 time: 1.906
2025-12-05 07:10:45,328: Train batch 11260: loss: 1.34 grad norm: 22.00 time: 1.594
2025-12-05 07:11:12,763: Train batch 11280: loss: 1.10 grad norm: 19.56 time: 1.333
2025-12-05 07:11:41,508: Train batch 11300: loss: 1.45 grad norm: 21.14 time: 1.308
2025-12-05 07:12:09,576: Train batch 11320: loss: 1.28 grad norm: 22.76 time: 1.272
2025-12-05 07:12:38,931: Train batch 11340: loss: 0.98 grad norm: 17.57 time: 1.533
2025-12-05 07:13:08,017: Train batch 11360: loss: 1.02 grad norm: 17.48 time: 1.317
2025-12-05 07:13:35,357: Train batch 11380: loss: 0.80 grad norm: 15.13 time: 1.320
2025-12-05 07:14:02,955: Train batch 11400: loss: 2.15 grad norm: 17.33 time: 1.591
2025-12-05 07:14:32,250: Train batch 11420: loss: 0.97 grad norm: 16.64 time: 1.575
2025-12-05 07:15:02,591: Train batch 11440: loss: 0.98 grad norm: 21.69 time: 1.488
2025-12-05 07:15:31,122: Train batch 11460: loss: 1.08 grad norm: 19.91 time: 1.679
2025-12-05 07:16:00,043: Train batch 11480: loss: 0.77 grad norm: 14.17 time: 1.376
2025-12-05 07:16:29,207: Train batch 11500: loss: 0.98 grad norm: 18.92 time: 1.498
2025-12-05 07:16:29,209: Running test after training batch: 11500
2025-12-05 07:16:36,212: Val batch 11500: PER (avg): 0.0943 CTC Loss (avg): 13.5995 time: 7.001
2025-12-05 07:16:36,212: t15.2023.08.13 val PER: 0.1195
2025-12-05 07:16:36,213: t15.2023.08.18 val PER: 0.0880
2025-12-05 07:16:36,213: t15.2023.08.20 val PER: 0.0866
2025-12-05 07:16:36,213: t15.2023.08.25 val PER: 0.0693
2025-12-05 07:16:36,213: t15.2023.08.27 val PER: 0.1399
2025-12-05 07:16:36,213: t15.2023.09.01 val PER: 0.0576
2025-12-05 07:16:36,213: t15.2023.09.03 val PER: 0.1461
2025-12-05 07:16:36,214: t15.2023.09.24 val PER: 0.0983
2025-12-05 07:16:36,214: t15.2023.09.29 val PER: 0.0868
2025-12-05 07:16:36,214: t15.2023.10.01 val PER: 0.0879
2025-12-05 07:16:36,214: t15.2023.10.06 val PER: 0.0861
2025-12-05 07:16:36,214: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 07:16:36,214: t15.2023.10.13 val PER: 0.0952
2025-12-05 07:16:36,214: t15.2023.10.15 val PER: 0.0864
2025-12-05 07:16:36,215: New best test PER 0.0948 --> 0.0943
2025-12-05 07:16:36,215: Checkpointing model
2025-12-05 07:16:39,675: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 07:17:08,148: Train batch 11520: loss: 0.85 grad norm: 17.02 time: 1.553
2025-12-05 07:17:37,520: Train batch 11540: loss: 0.67 grad norm: 16.23 time: 1.306
2025-12-05 07:18:07,557: Train batch 11560: loss: 0.93 grad norm: 20.19 time: 1.255
2025-12-05 07:18:36,240: Train batch 11580: loss: 0.89 grad norm: 21.55 time: 1.466
2025-12-05 07:19:02,818: Train batch 11600: loss: 0.76 grad norm: 17.05 time: 1.381
2025-12-05 07:19:31,488: Train batch 11620: loss: 0.67 grad norm: 14.40 time: 1.599
2025-12-05 07:20:00,053: Train batch 11640: loss: 0.65 grad norm: 15.76 time: 1.285
2025-12-05 07:20:28,016: Train batch 11660: loss: 1.13 grad norm: 18.88 time: 1.236
2025-12-05 07:20:56,350: Train batch 11680: loss: 0.74 grad norm: 16.15 time: 1.321
2025-12-05 07:21:26,319: Train batch 11700: loss: 1.34 grad norm: 23.80 time: 1.823
2025-12-05 07:21:56,036: Train batch 11720: loss: 1.25 grad norm: 21.68 time: 1.557
2025-12-05 07:22:25,490: Train batch 11740: loss: 0.99 grad norm: 19.87 time: 1.541
2025-12-05 07:22:55,256: Train batch 11760: loss: 0.65 grad norm: 15.56 time: 1.482
2025-12-05 07:23:24,073: Train batch 11780: loss: 0.62 grad norm: 15.23 time: 1.374
2025-12-05 07:23:52,859: Train batch 11800: loss: 0.92 grad norm: 21.04 time: 1.398
2025-12-05 07:24:22,744: Train batch 11820: loss: 0.83 grad norm: 16.85 time: 1.494
2025-12-05 07:24:50,392: Train batch 11840: loss: 0.76 grad norm: 16.08 time: 1.541
2025-12-05 07:25:19,549: Train batch 11860: loss: 0.56 grad norm: 13.23 time: 1.667
2025-12-05 07:25:48,191: Train batch 11880: loss: 0.67 grad norm: 16.00 time: 1.145
2025-12-05 07:26:17,404: Train batch 11900: loss: 0.66 grad norm: 15.50 time: 1.351
2025-12-05 07:26:45,520: Train batch 11920: loss: 0.94 grad norm: 22.76 time: 1.382
2025-12-05 07:27:14,825: Train batch 11940: loss: 0.55 grad norm: 13.51 time: 1.209
2025-12-05 07:27:41,857: Train batch 11960: loss: 1.13 grad norm: 16.91 time: 1.580
2025-12-05 07:28:09,621: Train batch 11980: loss: 0.57 grad norm: 14.91 time: 1.521
2025-12-05 07:28:39,254: Train batch 12000: loss: 0.76 grad norm: 15.42 time: 1.839
2025-12-05 07:28:39,257: Running test after training batch: 12000
2025-12-05 07:28:46,187: Val batch 12000: PER (avg): 0.0951 CTC Loss (avg): 13.6912 time: 6.930
2025-12-05 07:28:46,188: t15.2023.08.13 val PER: 0.1143
2025-12-05 07:28:46,188: t15.2023.08.18 val PER: 0.0889
2025-12-05 07:28:46,188: t15.2023.08.20 val PER: 0.0882
2025-12-05 07:28:46,188: t15.2023.08.25 val PER: 0.0708
2025-12-05 07:28:46,189: t15.2023.08.27 val PER: 0.1431
2025-12-05 07:28:46,189: t15.2023.09.01 val PER: 0.0633
2025-12-05 07:28:46,189: t15.2023.09.03 val PER: 0.1485
2025-12-05 07:28:46,189: t15.2023.09.24 val PER: 0.0947
2025-12-05 07:28:46,189: t15.2023.09.29 val PER: 0.0932
2025-12-05 07:28:46,189: t15.2023.10.01 val PER: 0.0912
2025-12-05 07:28:46,190: t15.2023.10.06 val PER: 0.0807
2025-12-05 07:28:46,190: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 07:28:46,190: t15.2023.10.13 val PER: 0.0935
2025-12-05 07:28:46,190: t15.2023.10.15 val PER: 0.0893
2025-12-05 07:29:13,229: Train batch 12020: loss: 0.98 grad norm: 17.14 time: 1.400
2025-12-05 07:29:42,511: Train batch 12040: loss: 0.76 grad norm: 13.93 time: 1.825
2025-12-05 07:30:11,029: Train batch 12060: loss: 1.43 grad norm: 24.95 time: 1.392
2025-12-05 07:30:39,896: Train batch 12080: loss: 0.79 grad norm: 19.47 time: 1.554
2025-12-05 07:31:07,551: Train batch 12100: loss: 0.86 grad norm: 18.85 time: 1.315
2025-12-05 07:31:35,896: Train batch 12120: loss: 1.20 grad norm: 20.22 time: 1.386
2025-12-05 07:32:04,038: Train batch 12140: loss: 1.34 grad norm: 15.71 time: 1.297
2025-12-05 07:32:33,343: Train batch 12160: loss: 0.69 grad norm: 12.96 time: 1.268
2025-12-05 07:33:03,132: Train batch 12180: loss: 1.31 grad norm: 25.66 time: 1.349
2025-12-05 07:33:32,455: Train batch 12200: loss: 1.03 grad norm: 22.38 time: 1.597
2025-12-05 07:34:00,353: Train batch 12220: loss: 0.85 grad norm: 20.91 time: 1.241
2025-12-05 07:34:28,111: Train batch 12240: loss: 0.88 grad norm: 20.13 time: 1.405
2025-12-05 07:34:57,700: Train batch 12260: loss: 0.48 grad norm: 13.21 time: 1.387
2025-12-05 07:35:26,318: Train batch 12280: loss: 0.98 grad norm: 19.89 time: 1.308
2025-12-05 07:35:56,178: Train batch 12300: loss: 0.83 grad norm: 15.55 time: 1.541
2025-12-05 07:36:24,111: Train batch 12320: loss: 0.56 grad norm: 13.30 time: 1.620
2025-12-05 07:36:53,311: Train batch 12340: loss: 0.88 grad norm: 19.25 time: 1.263
2025-12-05 07:37:21,470: Train batch 12360: loss: 0.47 grad norm: 13.23 time: 1.424
2025-12-05 07:37:52,052: Train batch 12380: loss: 1.21 grad norm: 22.99 time: 1.578
2025-12-05 07:38:20,686: Train batch 12400: loss: 0.73 grad norm: 18.58 time: 1.407
2025-12-05 07:38:49,422: Train batch 12420: loss: 0.67 grad norm: 14.75 time: 1.815
2025-12-05 07:39:17,892: Train batch 12440: loss: 0.61 grad norm: 14.24 time: 1.477
2025-12-05 07:39:47,728: Train batch 12460: loss: 0.71 grad norm: 16.16 time: 1.571
2025-12-05 07:40:16,337: Train batch 12480: loss: 0.77 grad norm: 21.50 time: 1.368
2025-12-05 07:40:45,492: Train batch 12500: loss: 0.99 grad norm: 19.86 time: 1.578
2025-12-05 07:40:45,495: Running test after training batch: 12500
2025-12-05 07:40:52,415: Val batch 12500: PER (avg): 0.0935 CTC Loss (avg): 13.8413 time: 6.918
2025-12-05 07:40:52,416: t15.2023.08.13 val PER: 0.1143
2025-12-05 07:40:52,416: t15.2023.08.18 val PER: 0.0880
2025-12-05 07:40:52,416: t15.2023.08.20 val PER: 0.0842
2025-12-05 07:40:52,416: t15.2023.08.25 val PER: 0.0678
2025-12-05 07:40:52,416: t15.2023.08.27 val PER: 0.1431
2025-12-05 07:40:52,416: t15.2023.09.01 val PER: 0.0601
2025-12-05 07:40:52,417: t15.2023.09.03 val PER: 0.1449
2025-12-05 07:40:52,417: t15.2023.09.24 val PER: 0.0947
2025-12-05 07:40:52,417: t15.2023.09.29 val PER: 0.0868
2025-12-05 07:40:52,417: t15.2023.10.01 val PER: 0.0879
2025-12-05 07:40:52,417: t15.2023.10.06 val PER: 0.0829
2025-12-05 07:40:52,417: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 07:40:52,418: t15.2023.10.13 val PER: 0.1016
2025-12-05 07:40:52,418: t15.2023.10.15 val PER: 0.0849
2025-12-05 07:40:52,418: New best test PER 0.0943 --> 0.0935
2025-12-05 07:40:52,418: Checkpointing model
2025-12-05 07:40:56,011: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 07:41:24,290: Train batch 12520: loss: 0.71 grad norm: 14.83 time: 1.592
2025-12-05 07:41:51,258: Train batch 12540: loss: 1.35 grad norm: 23.19 time: 1.549
2025-12-05 07:42:19,979: Train batch 12560: loss: 0.49 grad norm: 11.91 time: 1.835
2025-12-05 07:42:47,504: Train batch 12580: loss: 1.20 grad norm: 20.99 time: 1.399
2025-12-05 07:43:16,703: Train batch 12600: loss: 0.90 grad norm: 18.93 time: 1.646
2025-12-05 07:43:45,230: Train batch 12620: loss: 1.35 grad norm: 20.44 time: 1.307
2025-12-05 07:44:14,633: Train batch 12640: loss: 0.73 grad norm: 20.47 time: 1.388
2025-12-05 07:44:42,441: Train batch 12660: loss: 0.68 grad norm: 17.76 time: 1.436
2025-12-05 07:45:10,828: Train batch 12680: loss: 1.18 grad norm: 21.91 time: 1.516
2025-12-05 07:45:39,989: Train batch 12700: loss: 0.77 grad norm: 16.07 time: 1.214
2025-12-05 07:46:07,987: Train batch 12720: loss: 0.79 grad norm: 14.28 time: 1.209
2025-12-05 07:46:37,237: Train batch 12740: loss: 1.21 grad norm: 20.86 time: 1.331
2025-12-05 07:47:05,170: Train batch 12760: loss: 0.95 grad norm: 21.03 time: 1.563
2025-12-05 07:47:33,196: Train batch 12780: loss: 0.52 grad norm: 12.25 time: 1.469
2025-12-05 07:48:01,462: Train batch 12800: loss: 1.20 grad norm: 20.98 time: 1.746
2025-12-05 07:48:30,722: Train batch 12820: loss: 1.06 grad norm: 18.95 time: 1.486
2025-12-05 07:48:59,123: Train batch 12840: loss: 0.90 grad norm: 20.65 time: 1.486
2025-12-05 07:49:27,656: Train batch 12860: loss: 0.82 grad norm: 15.11 time: 1.616
2025-12-05 07:49:56,116: Train batch 12880: loss: 0.89 grad norm: 19.61 time: 1.255
2025-12-05 07:50:25,177: Train batch 12900: loss: 0.59 grad norm: 15.23 time: 1.375
2025-12-05 07:50:55,098: Train batch 12920: loss: 1.99 grad norm: 15.93 time: 1.609
2025-12-05 07:51:24,877: Train batch 12940: loss: 0.65 grad norm: 14.55 time: 1.220
2025-12-05 07:51:53,235: Train batch 12960: loss: 1.25 grad norm: 19.27 time: 1.361
2025-12-05 07:52:21,609: Train batch 12980: loss: 0.46 grad norm: 13.16 time: 1.436
2025-12-05 07:52:50,050: Train batch 13000: loss: 1.01 grad norm: 22.55 time: 1.731
2025-12-05 07:52:50,053: Running test after training batch: 13000
2025-12-05 07:52:57,052: Val batch 13000: PER (avg): 0.0938 CTC Loss (avg): 13.7673 time: 6.997
2025-12-05 07:52:57,053: t15.2023.08.13 val PER: 0.1206
2025-12-05 07:52:57,053: t15.2023.08.18 val PER: 0.0863
2025-12-05 07:52:57,053: t15.2023.08.20 val PER: 0.0898
2025-12-05 07:52:57,053: t15.2023.08.25 val PER: 0.0693
2025-12-05 07:52:57,054: t15.2023.08.27 val PER: 0.1399
2025-12-05 07:52:57,054: t15.2023.09.01 val PER: 0.0617
2025-12-05 07:52:57,054: t15.2023.09.03 val PER: 0.1378
2025-12-05 07:52:57,054: t15.2023.09.24 val PER: 0.1007
2025-12-05 07:52:57,054: t15.2023.09.29 val PER: 0.0804
2025-12-05 07:52:57,054: t15.2023.10.01 val PER: 0.0847
2025-12-05 07:52:57,055: t15.2023.10.06 val PER: 0.0818
2025-12-05 07:52:57,055: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 07:52:57,055: t15.2023.10.13 val PER: 0.0952
2025-12-05 07:52:57,055: t15.2023.10.15 val PER: 0.0893
2025-12-05 07:53:25,892: Train batch 13020: loss: 0.77 grad norm: 17.85 time: 1.673
2025-12-05 07:53:54,381: Train batch 13040: loss: 0.68 grad norm: 15.21 time: 1.840
2025-12-05 07:54:22,850: Train batch 13060: loss: 0.71 grad norm: 14.20 time: 1.351
2025-12-05 07:54:52,643: Train batch 13080: loss: 0.68 grad norm: 15.83 time: 1.453
2025-12-05 07:55:20,831: Train batch 13100: loss: 0.64 grad norm: 14.76 time: 1.599
2025-12-05 07:55:49,318: Train batch 13120: loss: 1.22 grad norm: 22.97 time: 1.407
2025-12-05 07:56:18,171: Train batch 13140: loss: 0.98 grad norm: 19.31 time: 1.284
2025-12-05 07:56:47,229: Train batch 13160: loss: 0.43 grad norm: 13.80 time: 1.612
2025-12-05 07:57:16,019: Train batch 13180: loss: 0.93 grad norm: 20.54 time: 1.260
2025-12-05 07:57:44,623: Train batch 13200: loss: 0.68 grad norm: 15.78 time: 1.352
2025-12-05 07:58:12,959: Train batch 13220: loss: 0.57 grad norm: 17.06 time: 1.281
2025-12-05 07:58:41,952: Train batch 13240: loss: 0.72 grad norm: 20.32 time: 1.537
2025-12-05 07:59:11,315: Train batch 13260: loss: 0.68 grad norm: 16.71 time: 1.395
2025-12-05 07:59:39,371: Train batch 13280: loss: 1.20 grad norm: 24.66 time: 1.375
2025-12-05 08:00:09,235: Train batch 13300: loss: 0.60 grad norm: 14.71 time: 1.546
2025-12-05 08:00:38,159: Train batch 13320: loss: 0.79 grad norm: 18.11 time: 1.362
2025-12-05 08:01:06,850: Train batch 13340: loss: 0.90 grad norm: 17.02 time: 1.510
2025-12-05 08:01:35,447: Train batch 13360: loss: 0.79 grad norm: 15.73 time: 1.809
2025-12-05 08:02:05,087: Train batch 13380: loss: 0.88 grad norm: 18.98 time: 1.413
2025-12-05 08:02:33,590: Train batch 13400: loss: 0.77 grad norm: 20.44 time: 1.391
2025-12-05 08:03:03,892: Train batch 13420: loss: 0.63 grad norm: 16.77 time: 1.635
2025-12-05 08:03:33,926: Train batch 13440: loss: 1.10 grad norm: 32.81 time: 1.251
2025-12-05 08:04:02,752: Train batch 13460: loss: 0.67 grad norm: 14.27 time: 1.630
2025-12-05 08:04:31,603: Train batch 13480: loss: 0.71 grad norm: 22.13 time: 1.583
2025-12-05 08:05:01,581: Train batch 13500: loss: 0.90 grad norm: 17.62 time: 1.482
2025-12-05 08:05:01,584: Running test after training batch: 13500
2025-12-05 08:05:08,573: Val batch 13500: PER (avg): 0.0931 CTC Loss (avg): 13.8694 time: 6.988
2025-12-05 08:05:08,573: t15.2023.08.13 val PER: 0.1123
2025-12-05 08:05:08,574: t15.2023.08.18 val PER: 0.0830
2025-12-05 08:05:08,574: t15.2023.08.20 val PER: 0.0913
2025-12-05 08:05:08,574: t15.2023.08.25 val PER: 0.0648
2025-12-05 08:05:08,574: t15.2023.08.27 val PER: 0.1383
2025-12-05 08:05:08,574: t15.2023.09.01 val PER: 0.0609
2025-12-05 08:05:08,575: t15.2023.09.03 val PER: 0.1401
2025-12-05 08:05:08,575: t15.2023.09.24 val PER: 0.1007
2025-12-05 08:05:08,575: t15.2023.09.29 val PER: 0.0868
2025-12-05 08:05:08,575: t15.2023.10.01 val PER: 0.0896
2025-12-05 08:05:08,575: t15.2023.10.06 val PER: 0.0840
2025-12-05 08:05:08,576: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 08:05:08,576: t15.2023.10.13 val PER: 0.0887
2025-12-05 08:05:08,576: t15.2023.10.15 val PER: 0.0893
2025-12-05 08:05:08,576: New best test PER 0.0935 --> 0.0931
2025-12-05 08:05:08,576: Checkpointing model
2025-12-05 08:05:12,251: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 08:05:40,015: Train batch 13520: loss: 0.68 grad norm: 16.65 time: 1.298
2025-12-05 08:06:07,448: Train batch 13540: loss: 0.67 grad norm: 16.64 time: 1.171
2025-12-05 08:06:37,521: Train batch 13560: loss: 0.90 grad norm: 20.51 time: 1.366
2025-12-05 08:07:06,802: Train batch 13580: loss: 2.08 grad norm: 24.02 time: 1.628
2025-12-05 08:07:36,049: Train batch 13600: loss: 1.02 grad norm: 20.90 time: 1.448
2025-12-05 08:08:05,713: Train batch 13620: loss: 1.06 grad norm: 21.52 time: 1.347
2025-12-05 08:08:35,181: Train batch 13640: loss: 1.11 grad norm: 20.58 time: 1.264
2025-12-05 08:09:04,219: Train batch 13660: loss: 0.83 grad norm: 19.68 time: 1.580
2025-12-05 08:09:32,877: Train batch 13680: loss: 0.59 grad norm: 15.60 time: 1.855
2025-12-05 08:10:01,265: Train batch 13700: loss: 1.38 grad norm: 24.93 time: 1.598
2025-12-05 08:10:30,654: Train batch 13720: loss: 0.69 grad norm: 13.96 time: 1.293
2025-12-05 08:11:00,538: Train batch 13740: loss: 0.63 grad norm: 17.30 time: 1.436
2025-12-05 08:11:28,986: Train batch 13760: loss: 0.49 grad norm: 10.60 time: 1.527
2025-12-05 08:11:58,622: Train batch 13780: loss: 0.95 grad norm: 19.06 time: 1.324
2025-12-05 08:12:26,280: Train batch 13800: loss: 1.21 grad norm: 22.20 time: 1.291
2025-12-05 08:12:54,896: Train batch 13820: loss: 0.69 grad norm: 16.13 time: 1.376
2025-12-05 08:13:24,215: Train batch 13840: loss: 0.57 grad norm: 13.67 time: 1.316
2025-12-05 08:13:52,684: Train batch 13860: loss: 0.53 grad norm: 14.83 time: 1.422
2025-12-05 08:14:21,457: Train batch 13880: loss: 0.85 grad norm: 18.17 time: 1.424
2025-12-05 08:14:50,605: Train batch 13900: loss: 0.77 grad norm: 17.17 time: 1.334
2025-12-05 08:15:19,085: Train batch 13920: loss: 0.70 grad norm: 14.18 time: 1.513
2025-12-05 08:15:46,688: Train batch 13940: loss: 0.76 grad norm: 14.38 time: 1.258
2025-12-05 08:16:16,263: Train batch 13960: loss: 0.74 grad norm: 15.05 time: 1.684
2025-12-05 08:16:44,778: Train batch 13980: loss: 1.10 grad norm: 21.32 time: 1.254
2025-12-05 08:17:12,743: Train batch 14000: loss: 0.51 grad norm: 10.38 time: 1.384
2025-12-05 08:17:12,746: Running test after training batch: 14000
2025-12-05 08:17:19,694: Val batch 14000: PER (avg): 0.0945 CTC Loss (avg): 14.0241 time: 6.947
2025-12-05 08:17:19,694: t15.2023.08.13 val PER: 0.1164
2025-12-05 08:17:19,694: t15.2023.08.18 val PER: 0.0838
2025-12-05 08:17:19,695: t15.2023.08.20 val PER: 0.0898
2025-12-05 08:17:19,695: t15.2023.08.25 val PER: 0.0633
2025-12-05 08:17:19,695: t15.2023.08.27 val PER: 0.1415
2025-12-05 08:17:19,695: t15.2023.09.01 val PER: 0.0625
2025-12-05 08:17:19,695: t15.2023.09.03 val PER: 0.1378
2025-12-05 08:17:19,695: t15.2023.09.24 val PER: 0.1019
2025-12-05 08:17:19,696: t15.2023.09.29 val PER: 0.0916
2025-12-05 08:17:19,696: t15.2023.10.01 val PER: 0.0977
2025-12-05 08:17:19,696: t15.2023.10.06 val PER: 0.0850
2025-12-05 08:17:19,696: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 08:17:19,696: t15.2023.10.13 val PER: 0.0919
2025-12-05 08:17:19,696: t15.2023.10.15 val PER: 0.0893
2025-12-05 08:17:48,082: Train batch 14020: loss: 0.79 grad norm: 18.32 time: 1.490
2025-12-05 08:18:16,561: Train batch 14040: loss: 0.47 grad norm: 11.40 time: 1.388
2025-12-05 08:18:45,084: Train batch 14060: loss: 0.34 grad norm: 10.98 time: 1.367
2025-12-05 08:19:13,987: Train batch 14080: loss: 0.47 grad norm: 14.74 time: 1.387
2025-12-05 08:19:42,878: Train batch 14100: loss: 1.18 grad norm: 19.79 time: 1.242
2025-12-05 08:20:12,334: Train batch 14120: loss: 1.02 grad norm: 21.56 time: 1.527
2025-12-05 08:20:40,686: Train batch 14140: loss: 0.63 grad norm: 15.30 time: 1.407
2025-12-05 08:21:09,712: Train batch 14160: loss: 0.98 grad norm: 22.41 time: 1.367
2025-12-05 08:21:39,604: Train batch 14180: loss: 0.96 grad norm: 19.19 time: 1.393
2025-12-05 08:22:08,351: Train batch 14200: loss: 0.62 grad norm: 12.30 time: 1.295
2025-12-05 08:22:37,804: Train batch 14220: loss: 0.91 grad norm: 20.63 time: 1.621
2025-12-05 08:23:06,086: Train batch 14240: loss: 1.06 grad norm: 23.17 time: 1.567
2025-12-05 08:23:35,738: Train batch 14260: loss: 1.00 grad norm: 19.28 time: 1.564
2025-12-05 08:24:05,224: Train batch 14280: loss: 0.70 grad norm: 18.00 time: 1.264
2025-12-05 08:24:33,674: Train batch 14300: loss: 0.52 grad norm: 12.59 time: 1.949
2025-12-05 08:25:02,086: Train batch 14320: loss: 0.51 grad norm: 11.19 time: 1.515
2025-12-05 08:25:31,730: Train batch 14340: loss: 0.75 grad norm: 16.58 time: 1.607
2025-12-05 08:25:59,949: Train batch 14360: loss: 0.60 grad norm: 13.78 time: 1.591
2025-12-05 08:26:28,412: Train batch 14380: loss: 0.83 grad norm: 17.75 time: 1.345
2025-12-05 08:26:56,750: Train batch 14400: loss: 0.82 grad norm: 25.32 time: 1.379
2025-12-05 08:27:24,718: Train batch 14420: loss: 0.74 grad norm: 19.37 time: 1.274
2025-12-05 08:27:53,263: Train batch 14440: loss: 0.85 grad norm: 15.33 time: 1.301
2025-12-05 08:28:20,474: Train batch 14460: loss: 0.52 grad norm: 12.78 time: 1.319
2025-12-05 08:28:48,888: Train batch 14480: loss: 0.29 grad norm: 7.50 time: 1.222
2025-12-05 08:29:18,000: Train batch 14500: loss: 0.52 grad norm: 14.08 time: 1.515
2025-12-05 08:29:18,002: Running test after training batch: 14500
2025-12-05 08:29:24,964: Val batch 14500: PER (avg): 0.0918 CTC Loss (avg): 13.8473 time: 6.961
2025-12-05 08:29:24,964: t15.2023.08.13 val PER: 0.1154
2025-12-05 08:29:24,965: t15.2023.08.18 val PER: 0.0847
2025-12-05 08:29:24,965: t15.2023.08.20 val PER: 0.0826
2025-12-05 08:29:24,965: t15.2023.08.25 val PER: 0.0663
2025-12-05 08:29:24,965: t15.2023.08.27 val PER: 0.1415
2025-12-05 08:29:24,966: t15.2023.09.01 val PER: 0.0601
2025-12-05 08:29:24,966: t15.2023.09.03 val PER: 0.1449
2025-12-05 08:29:24,966: t15.2023.09.24 val PER: 0.0922
2025-12-05 08:29:24,966: t15.2023.09.29 val PER: 0.0868
2025-12-05 08:29:24,966: t15.2023.10.01 val PER: 0.0879
2025-12-05 08:29:24,967: t15.2023.10.06 val PER: 0.0786
2025-12-05 08:29:24,967: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 08:29:24,967: t15.2023.10.13 val PER: 0.0887
2025-12-05 08:29:24,967: t15.2023.10.15 val PER: 0.0878
2025-12-05 08:29:24,967: New best test PER 0.0931 --> 0.0918
2025-12-05 08:29:24,967: Checkpointing model
2025-12-05 08:29:28,616: Saved model to checkpoint: trained_models/time_rnn/checkpoint/best_checkpoint
2025-12-05 08:29:57,534: Train batch 14520: loss: 0.51 grad norm: 13.27 time: 1.634
2025-12-05 08:30:27,511: Train batch 14540: loss: 0.75 grad norm: 15.34 time: 1.584
2025-12-05 08:30:56,489: Train batch 14560: loss: 0.84 grad norm: 19.43 time: 1.213
2025-12-05 08:31:26,274: Train batch 14580: loss: 0.83 grad norm: 19.92 time: 1.373
2025-12-05 08:31:54,822: Train batch 14600: loss: 0.67 grad norm: 14.47 time: 1.485
2025-12-05 08:32:23,659: Train batch 14620: loss: 0.57 grad norm: 12.65 time: 1.721
2025-12-05 08:32:54,044: Train batch 14640: loss: 0.96 grad norm: 22.35 time: 1.320
2025-12-05 08:33:23,304: Train batch 14660: loss: 0.63 grad norm: 13.61 time: 1.336
2025-12-05 08:33:52,690: Train batch 14680: loss: 0.95 grad norm: 19.59 time: 1.331
2025-12-05 08:34:22,117: Train batch 14700: loss: 1.33 grad norm: 26.43 time: 1.691
2025-12-05 08:34:50,190: Train batch 14720: loss: 0.57 grad norm: 14.86 time: 1.414
2025-12-05 08:35:19,779: Train batch 14740: loss: 0.47 grad norm: 13.13 time: 1.491
2025-12-05 08:35:49,227: Train batch 14760: loss: 0.70 grad norm: 17.84 time: 1.634
2025-12-05 08:36:17,466: Train batch 14780: loss: 1.05 grad norm: 26.04 time: 1.586
2025-12-05 08:36:47,012: Train batch 14800: loss: 0.66 grad norm: 14.53 time: 1.356
2025-12-05 08:37:15,374: Train batch 14820: loss: 1.21 grad norm: 17.01 time: 1.308
2025-12-05 08:37:43,697: Train batch 14840: loss: 0.66 grad norm: 17.08 time: 1.375
2025-12-05 08:38:12,280: Train batch 14860: loss: 0.69 grad norm: 19.51 time: 1.357
2025-12-05 08:38:38,707: Train batch 14880: loss: 0.80 grad norm: 19.71 time: 1.274
2025-12-05 08:39:06,850: Train batch 14900: loss: 0.33 grad norm: 7.88 time: 1.370
2025-12-05 08:39:34,814: Train batch 14920: loss: 0.76 grad norm: 14.41 time: 1.822
2025-12-05 08:40:02,935: Train batch 14940: loss: 1.00 grad norm: 20.73 time: 1.424
2025-12-05 08:40:30,282: Train batch 14960: loss: 0.74 grad norm: 16.61 time: 1.386
2025-12-05 08:40:59,057: Train batch 14980: loss: 0.54 grad norm: 19.02 time: 1.447
2025-12-05 08:41:29,511: Train batch 15000: loss: 0.80 grad norm: 15.48 time: 1.388
2025-12-05 08:41:29,513: Running test after training batch: 15000
2025-12-05 08:41:36,466: Val batch 15000: PER (avg): 0.0943 CTC Loss (avg): 14.2753 time: 6.951
2025-12-05 08:41:36,467: t15.2023.08.13 val PER: 0.1112
2025-12-05 08:41:36,467: t15.2023.08.18 val PER: 0.0889
2025-12-05 08:41:36,467: t15.2023.08.20 val PER: 0.0866
2025-12-05 08:41:36,467: t15.2023.08.25 val PER: 0.0693
2025-12-05 08:41:36,468: t15.2023.08.27 val PER: 0.1415
2025-12-05 08:41:36,468: t15.2023.09.01 val PER: 0.0576
2025-12-05 08:41:36,468: t15.2023.09.03 val PER: 0.1401
2025-12-05 08:41:36,468: t15.2023.09.24 val PER: 0.0995
2025-12-05 08:41:36,468: t15.2023.09.29 val PER: 0.0916
2025-12-05 08:41:36,469: t15.2023.10.01 val PER: 0.1026
2025-12-05 08:41:36,469: t15.2023.10.06 val PER: 0.0872
2025-12-05 08:41:36,469: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 08:41:36,469: t15.2023.10.13 val PER: 0.0919
2025-12-05 08:41:36,469: t15.2023.10.15 val PER: 0.0849
2025-12-05 08:42:05,601: Train batch 15020: loss: 0.68 grad norm: 16.13 time: 1.810
2025-12-05 08:42:33,530: Train batch 15040: loss: 0.70 grad norm: 16.62 time: 1.300
2025-12-05 08:43:01,222: Train batch 15060: loss: 0.47 grad norm: 11.31 time: 1.255
2025-12-05 08:43:30,089: Train batch 15080: loss: 0.56 grad norm: 13.98 time: 1.232
2025-12-05 08:43:59,026: Train batch 15100: loss: 0.75 grad norm: 19.95 time: 1.408
2025-12-05 08:44:28,700: Train batch 15120: loss: 0.70 grad norm: 21.45 time: 1.284
2025-12-05 08:44:57,198: Train batch 15140: loss: 0.80 grad norm: 17.77 time: 1.923
2025-12-05 08:45:26,433: Train batch 15160: loss: 3.22 grad norm: 21.30 time: 1.671
2025-12-05 08:45:55,457: Train batch 15180: loss: 0.54 grad norm: 14.53 time: 1.612
2025-12-05 08:46:24,301: Train batch 15200: loss: 0.81 grad norm: 16.30 time: 1.396
2025-12-05 08:46:51,963: Train batch 15220: loss: 0.54 grad norm: 20.74 time: 1.250
2025-12-05 08:47:19,790: Train batch 15240: loss: 0.50 grad norm: 13.62 time: 1.292
2025-12-05 08:47:48,388: Train batch 15260: loss: 0.75 grad norm: 19.98 time: 1.528
2025-12-05 08:48:17,666: Train batch 15280: loss: 0.52 grad norm: 12.84 time: 1.592
2025-12-05 08:48:47,203: Train batch 15300: loss: 0.80 grad norm: 19.44 time: 1.585
2025-12-05 08:49:16,010: Train batch 15320: loss: 0.85 grad norm: 19.24 time: 1.544
2025-12-05 08:49:45,778: Train batch 15340: loss: 0.66 grad norm: 14.74 time: 1.327
2025-12-05 08:50:15,267: Train batch 15360: loss: 0.60 grad norm: 15.57 time: 1.396
2025-12-05 08:50:43,531: Train batch 15380: loss: 0.62 grad norm: 15.36 time: 1.280
2025-12-05 08:51:12,881: Train batch 15400: loss: 0.76 grad norm: 15.44 time: 1.419
2025-12-05 08:51:42,920: Train batch 15420: loss: 0.48 grad norm: 14.18 time: 1.602
2025-12-05 08:52:12,139: Train batch 15440: loss: 0.61 grad norm: 16.82 time: 1.452
2025-12-05 08:52:40,386: Train batch 15460: loss: 0.43 grad norm: 11.48 time: 1.274
2025-12-05 08:53:08,987: Train batch 15480: loss: 0.56 grad norm: 15.64 time: 1.597
2025-12-05 08:53:39,490: Train batch 15500: loss: 0.58 grad norm: 14.44 time: 1.435
2025-12-05 08:53:39,492: Running test after training batch: 15500
2025-12-05 08:53:46,454: Val batch 15500: PER (avg): 0.0925 CTC Loss (avg): 14.1894 time: 6.960
2025-12-05 08:53:46,454: t15.2023.08.13 val PER: 0.1123
2025-12-05 08:53:46,455: t15.2023.08.18 val PER: 0.0855
2025-12-05 08:53:46,455: t15.2023.08.20 val PER: 0.0874
2025-12-05 08:53:46,455: t15.2023.08.25 val PER: 0.0663
2025-12-05 08:53:46,455: t15.2023.08.27 val PER: 0.1463
2025-12-05 08:53:46,455: t15.2023.09.01 val PER: 0.0609
2025-12-05 08:53:46,456: t15.2023.09.03 val PER: 0.1366
2025-12-05 08:53:46,456: t15.2023.09.24 val PER: 0.1007
2025-12-05 08:53:46,456: t15.2023.09.29 val PER: 0.0820
2025-12-05 08:53:46,456: t15.2023.10.01 val PER: 0.0879
2025-12-05 08:53:46,456: t15.2023.10.06 val PER: 0.0775
2025-12-05 08:53:46,456: t15.2023.10.08 val PER: N/A (no validation sequences)
2025-12-05 08:53:46,457: t15.2023.10.13 val PER: 0.0952
2025-12-05 08:53:46,457: t15.2023.10.15 val PER: 0.0878
2025-12-05 08:54:15,233: Train batch 15520: loss: 0.74 grad norm: 16.08 time: 1.352
