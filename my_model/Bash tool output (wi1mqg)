================================================================================
QWEN2-AUDIO-7B ARCHITECTURE DIMENSIONS
================================================================================

ðŸ“Š AUDIO PROCESSING PATH:
  Raw Audio â†’ Whisper Encoder â†’ AudioTower â†’ Projector â†’ LLM

  1. Whisper Encoder (frozen, from openai/whisper-large-v3)
     Input:  Audio waveform (16kHz)
     Output: [batch, T_audio, 1280]
     â†’ This is the AudioTower output

  2. Multi-Modal Projector (frozen)
     Input:  [batch, T_audio, 1280]
     Output: [batch, T_audio, 4096]
     â†’ Projects to LLM embedding space

  3. Qwen2-7B Language Model (frozen)
     Input:  [batch, T, 4096]
     Output: [batch, T, vocab_size=152064]

================================================================================
YOUR BRAIN ENCODER PATHWAY
================================================================================

  Raw Brain â†’ Day Layers â†’ GRU â†’ Projector â†’ (alignment/LLM)

  1. Raw Brain Data
     Shape: [batch, T_brain, 512]
     â†’ 512 neural features per timestep

  2. Day-Specific Layers
     Shape: [batch, T_brain, 512] â†’ [batch, T_brain, 512]
     â†’ Linear transform + bias per day (calibration)

  3. GRU Encoder (n_layers=5, n_units=768)
     Shape: [batch, T_brain, 512] â†’ [batch, T_brain, 768]
     â†’ Temporal modeling

  4. Projector
     Shape: [batch, T_brain, 768] â†’ [batch, T_brain, ???]
     â†’ PROJECT TO WHAT DIMENSION?

================================================================================
CRITICAL QUESTION: WHAT SHOULD PROJECTOR OUTPUT?
================================================================================

You have TWO objectives:

  Objective 1: Align with AudioTower embeddings
    â†’ Brain should match AudioTower output: 1280 dims
    â†’ For alignment loss: MSE(brain_emb, audio_emb)

  Objective 2: Feed into LLM Decoder
    â†’ LLM expects input from Projector: 4096 dims
    â†’ But you're using the frozen Qwen Projector: 1280 â†’ 4096

DECISION TREE:

  Option A: Brain Encoder â†’ 1280 dims (CURRENT)
    âœ“ Aligns directly with AudioTower output
    âœ“ Can compute alignment loss easily
    âœ“ Then pass through frozen Qwen Projector (1280 â†’ 4096) for LLM
    âœ“ MATCHES YOUR CURRENT ARCHITECTURE

  Option B: Brain Encoder â†’ 4096 dims
    âœ— Can't align with AudioTower (1280 != 4096)
    âœ— Would need to also project AudioTower (1280 â†’ 4096)
    âœ— Bypasses the frozen Qwen Projector
    âœ— NOT RECOMMENDED

================================================================================
RECOMMENDATION
================================================================================

âœ… KEEP: Brain Encoder â†’ 1280 dimensions

Why?
  1. You want to align brain with AUDIO, not with LLM embeddings
  2. AudioTower outputs 1280 dims (Whisper-large-v3)
  3. Qwen's frozen Projector handles 1280 â†’ 4096 conversion
  4. You get the benefit of Qwen's learned projection

Architecture:
  Brain [512] â†’ GRU [768] â†’ Projector [1280]
                                  â†“
                         Alignment Loss (with Audio [1280])
                                  â†“
                    Qwen Projector [1280 â†’ 4096]
                                  â†“
                         LLM [4096 â†’ text]

================================================================================
DOES INCREASING 512â†’1280 MAKE SENSE?
================================================================================

YES! Here's why:

  Input: 512 dimensions
    - 512 electrode features
    - Each dimension is noisy, redundant
    - Local information, not semantic

  GRU: 768 hidden units
    - Integrates temporal context
    - Learns patterns over time
    - Extracts higher-level features
    - 768 > 512: EXPANSION happens here

  Projector: 768 â†’ 1280
    - Further expansion for semantic space
    - 1280 dims matches rich audio embedding space
    - Provides capacity for complex alignment

This is NOT just 'upsampling' - it's FEATURE EXTRACTION:
  - 512 dims: Raw, noisy neural signals
  - 768 dims: Temporal features from GRU
  - 1280 dims: Semantic features for alignment

Compare to computer vision:
  - Image: 224x224x3 = 150k input dims
  - ResNet: Compresses to 2048 dims
  - Then expands to 512 dims for classification
  â†’ Expansion after feature extraction is NORMAL!

================================================================================
SHOULD YOU SQUASH LAST 2 DIMENSIONS?
================================================================================

NO! Keep GRU [768] â†’ Projector [1280] separate:

  Why separate?
    1. GRU learns temporal dynamics (recurrent)
    2. Projector learns semantic mapping (feedforward)
    3. Different roles, different inductive biases

  Alternative (squashed):
    GRU [1280] â†’ No Projector
    âœ— GRU is overkill for just dimension matching
    âœ— Harder to train (more parameters in recurrent layers)
    âœ— Loses the interpretability of separate stages

  Current (separate):
    GRU [768] â†’ Linear [1280]
    âœ“ GRU focuses on temporal modeling
    âœ“ Projector focuses on space alignment
    âœ“ Easier to tune/debug each component
    âœ“ RECOMMENDED

================================================================================
FINAL ARCHITECTURE VALIDATION
================================================================================

Stage 1: Brain Encoding
  [batch, T, 512] â†’ Day Layers â†’ [batch, T, 512]
  [batch, T, 512] â†’ GRU(768)  â†’ [batch, T, 768]
  [batch, T, 768] â†’ Linear    â†’ [batch, T, 1280] âœ“

Stage 2: Audio Encoding (frozen)
  text â†’ TTS â†’ audio â†’ Whisper â†’ [batch, T, 1280] âœ“

Stage 3: Alignment
  Brain [1280] âŸ· Audio [1280]
  â†’ MSE Loss âœ“

Stage 4: LLM Decoding (frozen)
  Brain [1280] â†’ Qwen Proj â†’ [1280 â†’ 4096]
  [batch, T, 4096] â†’ Qwen LLM â†’ text âœ“

ALL DIMENSIONS ALIGNED! âœ…