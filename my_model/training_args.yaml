# Brain-to-Text Model Training Configuration
# Based on baseline RNN model configuration with adaptations for audio-LLM alignment

# Audio embedding cache (REQUIRED - run precompute_all_embeddings.py first!)
cache_dir: /home/ubuntu/my_model/cache/audio_embeddings  # Directory containing precomputed audio embeddings

model:
  n_input_features: 512  # 2 features per electrode × 256 electrodes
  n_units: 768  # GRU hidden units
  rnn_dropout: 0.4  # dropout rate for GRU layers
  rnn_trainable: true  # whether GRU layers are trainable
  n_layers: 4  # number of GRU layers
  patch_size: 14  # size of input patches (timesteps)
  patch_stride: 4  # stride for input patches
  audio_embedding_dim: 1280  # Qwen AudioTower output dimension

  input_network:
    n_input_layers: 1  # one network per day
    input_layer_sizes:
    - 512  # size matches neural_dim
    input_trainable: true  # whether input layer is trainable
    input_layer_dropout: 0.2  # dropout rate for input layer

  # Audio-LLM components
  t2a_model_id: "facebook/mms-tts-eng"  # Text-to-Audio model
  a2t_model_id: "Qwen/Qwen2-Audio-7B-Instruct"  # Audio-to-Text LLM

  # Quantization settings (set use_quantization: true to save GPU memory)
  use_quantization: False
  quantization_bits: 8  # 4-bit or 8-bit quantization

  # LoRA (Low-Rank Adaptation) settings for efficient LLM fine-tuning
  use_lora: true  # Enable LoRA adapters on the LLM
  lora_r: 8  # LoRA rank (higher = more parameters, better capacity)
  lora_alpha: 16  # LoRA scaling factor (usually 2x rank)
  lora_dropout: 0.1  # Dropout for LoRA layers
  lora_target_modules:  # Which modules to apply LoRA to
    - q_proj  # Query projection in attention
    - v_proj  # Value projection in attention
    - k_proj  # Key projection in attention
    - o_proj  # Output projection in attention
    - gate_proj  # Gate projection in FFN
    - up_proj  # Up projection in FFN
    - down_proj  # Down projection in FFN
  lora_bias: none  # How to handle bias terms: 'none', 'all', or 'lora_only'

# Training parameters
gpu_number: '0'  # GPU to use (string format) - ignored if use_multi_gpu is true
use_multi_gpu: False  # Enable multi-GPU training with DataParallel 
mode: train
use_amp: true  # automatic mixed precision (bfloat16)

output_dir: trained_models/brain2text_audio_llm  # output directory
checkpoint_dir: trained_models/brain2text_audio_llm/checkpoint  # checkpoint directory
init_from_checkpoint: false  # load from checkpoint
init_checkpoint_path: null  # checkpoint path if loading
save_best_checkpoint: true  # save best checkpoint
save_all_val_steps: false  # save all validation checkpoints
save_final_model: true  # save final model
save_val_metrics: true  # save validation metrics
early_stopping: true  # use early stopping
early_stopping_val_steps: 7  # stop if no improvement for N validation steps

num_training_batches: 220000  # total training batches
lr_scheduler_type: cosine  # 'cosine' or 'linear'

# Learning rates - FIXED for larger dataset
lr_max: 0.005  # max LR for main model
lr_min: 0.0001  # min LR for main model
lr_decay_steps: 120000  # LR decay duration (restored from original)
lr_warmup_steps: 1000  # warmup steps (restored from original)
lr_max_day: 0.005  # max LR for day-specific layers
lr_min_day: 0.0001  # min LR for day-specific layers
lr_decay_steps_day: 120000  # LR decay for day layers (restored)
lr_warmup_steps_day: 1000  # warmup for day layers (restored)

# Optimizer parameters
weight_decay: 0.001  # weight decay for main model
weight_decay_day: 0  # no weight decay for day-specific layers
seed: 10  # random seed
grad_norm_clip_value: 10  # gradient clipping

# Loss weights - REBALANCED to prioritize alignment
alpha: 3.0  # alignment loss weight (brain ↔ audio) - INCREASED from 1.0
beta: 1.0  # LLM loss weight (text generation) - kept at 1.0

# Logging and evaluation
batches_per_train_log: 200  # log every N batches
batches_per_val_step: 2000  # validate every N batches
log_individual_day_val_metrics: true  # log per-day metrics
save_val_logits: false  # don't save logits (too large for LLM)
save_val_data: false  # don't save validation data

# Dataset configuration
dataset:
  # Data augmentation (applied on GPU)
  data_transforms:
    white_noise_std: 1.0  # white noise augmentation
    constant_offset_std: 0.2  # constant offset per trial
    random_walk_std: 0.0  # random walk noise (disabled)
    random_walk_axis: -1  # axis for random walk
    static_gain_std: 0.0  # static gain noise (disabled)
    random_cut: 3  # randomly cut 0-3 timesteps from start
    smooth_kernel_size: 100  # Gaussian smoothing kernel size
    smooth_data: true  # CRITICAL: always smooth neural data
    smooth_kernel_std: 2  # Gaussian kernel std dev

  neural_dim: 512  # neural data dimensionality
  batch_size: 16  # batch size
  days_per_batch: 4  # mix data from N days per batch
  seed: 1  # dataset random seed
  num_dataloader_workers: 2  # workers for dataloader (0 for audio generation)
  loader_shuffle: false  # dataset handles shuffling internally
  must_include_days: null  # force specific days in each batch
  feature_subset: null  # use subset of features (null = all)

  dataset_dir: /home/ubuntu/data/hdf5_data_final # directory containing the dataset
  bad_trials_dict: null # dictionary of bad trials to exclude from the dataset
  sessions: # list of sessions to include in the dataset
  - t15.2023.08.11
  - t15.2023.08.13
  - t15.2023.08.18
  - t15.2023.08.20
  - t15.2023.08.25
  - t15.2023.08.27
  - t15.2023.09.01
  - t15.2023.09.03
  - t15.2023.09.24
  - t15.2023.09.29
  - t15.2023.10.01
  - t15.2023.10.06
  - t15.2023.10.08
  - t15.2023.10.13
  - t15.2023.10.15
  - t15.2023.10.20
  - t15.2023.10.22
  - t15.2023.11.03
  - t15.2023.11.04
  - t15.2023.11.17
  - t15.2023.11.19
  - t15.2023.11.26
  - t15.2023.12.03
  - t15.2023.12.08
  - t15.2023.12.10
  - t15.2023.12.17
  - t15.2023.12.29
  - t15.2024.02.25
  - t15.2024.03.03
  - t15.2024.03.08
  - t15.2024.03.15
  - t15.2024.03.17
  - t15.2024.04.25
  - t15.2024.04.28
  - t15.2024.05.10
  - t15.2024.06.14
  - t15.2024.07.19
  - t15.2024.07.21
  - t15.2024.07.28
  - t15.2025.01.10
  - t15.2025.01.12
  - t15.2025.03.14
  - t15.2025.03.16
  - t15.2025.03.30
  - t15.2025.04.13

  dataset_probability_val: # probability of including a trial in the validation set (0 or 1)
  - 0 # no val or test data from this day
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 0 # no val or test data from this day
  - 1
  - 1
  - 1
  - 0 # no val or test data from this day
  - 0 # no val or test data from this day
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1
  - 1



  